{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Linear Regression Models and Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model\n",
    "\n",
    "- Form of the linear regression model: *$f(X)=\\beta_{0}+\\sum_{j=1}^{p}X_{j}\\beta_{j}$*.\n",
    "\n",
    "- Training data: (x1,y1) ... (xN,yN). Each $x_{i} =(x_{i1},x_{i2},...,x_{ip})^{T}$ is a vector of feature measurements for the ith case. \n",
    "\n",
    "- Goal: estimate the parameters β\n",
    "\n",
    "- Estimation method: **Least Squares**, we pick the coeﬃcients $β =(β0,β1,...,βp)^{T}$ to minimize the **residual sum of squares**\n",
    "\n",
    "**Assumptions:**\n",
    "\n",
    "- Observations $y_i$ are uncorrelated and have constant variance $\\sigma^2$;\n",
    "- $x_i$ are ﬁxed (non random)\n",
    "- The regression function E(Y |X) is linear, or the linear model is a reasonable approximation.\n",
    "\n",
    "\n",
    "### Residual Sum of Squares\n",
    "\\begin{align}\n",
    "RSS(\\beta)=\\sum_{i=1}^{N}(y_{i}-f(x_{i}))^2=\\sum_{i=1}^{N}(y_{i}-\\beta_{0}-\\sum_{j=1}^{p}X_{ij}\\beta_{j})^2\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "#### Solution\n",
    "Denote by X the N × (p + 1) matrix with each row an input vector (with a 1 in the ﬁrst position), and similarly let y be the N-vector of outputs in the training set.\n",
    "\n",
    "\\begin{align} \\min RSS(\\beta)= (y-\\mathbf{X}\\beta)^T(y-\\mathbf{X}\\beta) \\end{align}\n",
    "\n",
    "A quadratic function in the p + 1 parameters\n",
    "\n",
    "Taking derivatives:\n",
    "\n",
    "\\begin{align} \\frac{\\partial RSS}{\\partial \\beta}=-2\\mathbf{X}^T(y-\\mathbf{X}\\beta) \\end{align}\n",
    "\n",
    "\\begin{align} \\frac{\\partial^2 RSS}{\\partial \\beta \\partial \\beta^T}=2\\mathbf{X}^T\\mathbf{X}  \\end{align}\n",
    "\n",
    "Assuming (for the moment) that $\\mathbf{X}$ has full column rank, and hence $\\mathbf{X}^T\\mathbf{X}$ is positive deﬁnite, we set the ﬁrst derivative to zero: $\\mathbf{X}^T(y-\\mathbf{X}\\beta)=0$\n",
    "\n",
    "\\begin{align}\\Rightarrow \\hat{\\beta}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty \\end{align}\n",
    "\n",
    "Fitted values at the training inputs: $\\hat{y}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$\n",
    "\n",
    "Hat matrix: $H=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$\n",
    "\n",
    "### Sampling Properties of $\\beta$\n",
    "\n",
    "The variance–covariance matrix of the least squares parameter estimates: \n",
    "\\begin{align} Var(\\hat{\\beta})=(\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2) \\end{align}\n",
    "\n",
    "Unbiased estimate of $\\sigma^2$: \n",
    "\\begin{align} \\hat{\\sigma^2}=\\frac{1}{N-p-1}\\sum^{N}_{i=1}(y_i-\\hat{y_i})^2 \\end{align}\n",
    "\n",
    "Assume the deviations of $\\mathbf{Y}$ around its expectation are additive and Gaussian:\n",
    "\\begin{align} Y=E(Y|X_1,...,X_p)+\\epsilon=\\beta_0+\\sum_{j=1}^{p}X_j\\beta_j+\\epsilon \\end{align}\n",
    "\n",
    "where $\\epsilon \\sim N(0,\\sigma^2)$\n",
    "\n",
    "Thus, $\\beta$ follows multivariate normal distribution with mean vector and variance–covariance matrix:\n",
    "\\begin{align}\\hat{\\beta} \\sim N(\\beta,(\\mathbf{X}^T\\mathbf{X})^{-1}\\sigma^2 ) \\end{align}\n",
    "\n",
    "Also, a chi-squared distribution with N −p−1 degrees of freedom:\n",
    "\\begin{align} (N-p-1)\\hat{\\sigma^2} \\sim \\sigma^2 \\chi_{N-p-1}^{2} \\end{align}\n",
    "\n",
    "($\\hat{\\beta}$ and $\\hat{\\sigma^2}$ are indep.)\n",
    "\n",
    "We use these distributional properties to form tests of hypothesis and conﬁdence intervals for the parameters $\\beta_j$:\n",
    "\n",
    "\n",
    "\n",
    "### Inference\n",
    "\n",
    "#### Test hypothesis\n",
    "\n",
    "To test the hypothesis that a particular coeﬃcient $\\beta_j= 0$, we form thestandardized coeﬃcient or Z-score:\n",
    "\\begin{align} z_j=\\frac{\\hat{\\beta_j}}{\\hat{\\sigma}\\sqrt{\\upsilon_j}} \\end{align}\n",
    "\n",
    "where $\\upsilon_j$ is the jth diagonal element of $(\\mathbf{X}^T\\mathbf{X})^{-1}$\n",
    "\n",
    "Under the null hypothesis that $\\beta_j= 0$, $z_j$ is distributed as $t_{N-p-1}$, and hence a large (absolute) value of zj will lead to\n",
    "rejection of this null hypothesis. If $\\hat{\\sigma}$ is replaced by a known value σ, then $z_j$ would have a standard normal distribution.\n",
    "\n",
    "#### Test the signiﬁcance of groups of coeﬃcients\n",
    "\n",
    "To test if a categorical variable with k levels can be excluded from a model, we need to test whether the coeﬃcients of the dummy variables used to represent the levels can all be set to zero. Here we use the F statistic：\n",
    "\n",
    "\\begin{align} F=\\frac{(RSS_0-RSS_1)/(p_1-p_0)}{RSS_1/(N-p_1-1)} \\end{align}\n",
    "\n",
    "- RSS1 is the residual sum-of-squares for the least squares ﬁt of the bigger model with p1+1 parameters;\n",
    "- RSS0 the same for the nested smaller model with p0 +1 parameters, having p1 −p0 parameters constrained to be zero.\n",
    "\n",
    "The F statistic measures the change in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an estimate of $\\sigma^2$.\n",
    "\n",
    "Under the Gaussian assumptions, and the null hypothesis that the smaller model is correct, the F statistic will have a $F_{p_1-p_0,N-p_1-1}$ distribution. \n",
    "\n",
    "### Confidence Interval\n",
    "\n",
    "$1-2\\alpha$ conﬁdence interval for $\\beta_j$:\n",
    "\\begin{align} (\\hat{\\beta_j}-z^{(1-\\alpha)}\\upsilon^{0.5}_j \\hat{\\sigma},\\hat{\\beta_j}+z^{(1-\\alpha)}\\upsilon^{0.5}_j \\hat{\\sigma}) \\end{align}\n",
    "where $z^{(1-\\alpha)}$ is the 1 − α percentile of the normal distribution. $\\alpha$ could be 0.025, 0.5, etc.\n",
    "\n",
    "In a similar fashion we can obtain an approximate confidence set for the entire parameter vector $\\beta$, namely\n",
    "\n",
    "\\begin{equation}\n",
    "C_\\beta = \\left\\{ \\beta \\big| (\\hat\\beta-\\beta)^T\\mathbf{X}^T\\mathbf{X}(\\hat\\beta-\\beta) \\le \\hat\\sigma^2{\\chi^2_{p+1}}^{(1-\\alpha)}\\right\\},\n",
    "\\end{equation}\n",
    "\n",
    "This condifence set for $\\beta$ generates a corresponding confidence set for the true function $f(x) = x^T\\beta$: $\\left\\{ x^T\\beta \\big| \\beta \\in C_\\beta \\right\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.2.1 Example: Prostate Cancer\n",
    "\n",
    "See other notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 The Gauss–Markov Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on estimation of any linear combination of the parameters $\\theta=\\alpha^T\\beta$, for example, predictions $f(x_0)=x^T_0\\beta$ are of this form.The least squares estimate of $\\alpha^T\\beta$ is:\n",
    "\\begin{equation}\n",
    "\\hat{\\theta}=\\alpha^T\\hat{\\beta}=\\alpha^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "Considering $\\mathbf{X}$ to be ﬁxed, this is a linear function $\\mathbf{c}^T_0\\mathbf{y}$ of the response vector $\\mathbf{y}$.If we assume that the linear model is correct, $\\alpha^T\\hat{\\beta}$ is unbiased.\n",
    "\n",
    "The Gauss–Markov theorem states that if we have any other linear estimator $\\tilde{\\theta}=\\mathbf{c}^T\\mathbf{y}$ that is unbiased for $\\alpha^T\\beta$, that is, $E(\\mathbf{c}^T\\mathbf{y})= \\alpha^T\\beta$, then:\n",
    "\\begin{equation}\n",
    "Var(\\alpha^T\\hat{\\beta})\\leq Var(\\mathbf{c}^T\\mathbf{y})\n",
    "\\end{equation}\n",
    "\n",
    "### Variance & Bias\n",
    "Consider the mean squared error of an estimator $\\tilde{\\theta}$ in estimating $\\theta$:\n",
    "\n",
    "\\begin{align}\n",
    "MSE(\\tilde{\\theta})&= E(\\tilde{\\theta}-\\theta)^2 \\\\\n",
    "&= E(\\tilde{\\theta^2}+\\theta^2-2\\theta\\tilde{\\theta}) \\\\\n",
    "&= E(\\tilde{\\theta^2})-E^2(\\tilde{\\theta})+E^2(\\tilde{\\theta})+E(\\theta^2-2\\theta\\tilde{\\theta})\\\\\n",
    "&= Var(\\tilde{\\theta})+[E(\\tilde{\\theta})-\\theta]^2\n",
    "\\end{align}\n",
    "\n",
    "The ﬁrst term is the **variance**, while the second term is the **squared bias**.\n",
    "\n",
    "The **Gauss-Markov theorem** implies that the *least squares estimator* has the smallest mean squared error of all linear estimators with no bias. However, there may well exist a biased estimator with smaller mean squared error. <font color='red'>Such an estimator would trade a little bias for a larger reduction in variance.</font>\n",
    "\n",
    "From a more pragmatic point of view, most models are distortions of the truth, and hence are biased; picking the right model amounts to creating the right balance between bias and variance.\n",
    "\n",
    "Mean squared error is intimately related to prediction accuracy. Consider the prediction of the new response at input $x_0$,\n",
    "\\begin{equation}\n",
    "y_0=f(x_0)+\\epsilon_0\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Prediction error & MSE\n",
    "The expected prediction error of an estimate $\\tilde{f}(x_0)=x_0^T\\tilde{\\beta}$:\n",
    "\n",
    "\\begin{align}\n",
    "E(y_0-\\tilde{f}(x_0))^2 &=E(f(x_0)+\\epsilon_0-x_0^T\\tilde{\\beta})^2 \\\\\n",
    "&=E(\\epsilon_0^2)+E(f(x_0)-x_0^T\\tilde{\\beta})^2-2E(\\epsilon_0(f(x_0)-x_0^T\\tilde{\\beta})) \\\\\n",
    "&=\\sigma^2+E(f(x_0)-x_0^T\\tilde{\\beta})^2 \\\\\n",
    "&=\\sigma^2+MSE(\\tilde{f}(x_0))\n",
    "\\end{align}\n",
    "\n",
    "Therefore, expected prediction error and mean squared error diﬀer only by the constant $\\sigma^2$, representing the variance of the new observation $y_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression from Simple Univariate Regression\n",
    "\n",
    "Gram-Schmidt正交化\n",
    "\n",
    "### Simple Univariate Regression\n",
    "Suppose ﬁrst that we have a univariate model with no intercept:\n",
    "\\begin{align}\n",
    "\\mathbf{Y}=\\mathbf{X}\\beta+\\epsilon\n",
    "\\end{align}\n",
    "The least squares estimate and residuals are:\n",
    "\\begin{align}\n",
    "\\hat{\\beta}&=\\frac{\\sum_1^Nx_iy_i}{\\sum_1^Nx_i^2} \\\\\n",
    "r_i&=y_i-x_i\\hat{\\beta}\n",
    "\\end{align}\n",
    "\n",
    "Let $\\mathbf{y}=(y_1,...,y_N)^T$,$\\mathbf{x}=(x_1,...,x_N)^T$\n",
    "\n",
    "Define the **inner product** between $\\mathbf{x}$ and  $\\mathbf{y}$:\n",
    "\\begin{align}\n",
    "<\\mathbf{x},\\mathbf{y}>=\\sum_1^Nx_iy_i=\\mathbf{x}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "Then,\n",
    "\\begin{align}\n",
    "\\hat{\\beta}&=\\frac{<\\mathbf{x},\\mathbf{y}>}{<\\mathbf{x},\\mathbf{x}>} \\\\\n",
    "\\mathbf{r}&=\\mathbf{y}-\\mathbf{x}\\hat{\\beta}\n",
    "\\end{align}\n",
    "\n",
    "Suppose the inputs x1, x2,..., xp (the columns of the data matrix $\\mathbf{X}$) are orthogonal; that is $<\\mathbf{x_k},\\mathbf{x_j}>=0$. Then the multiple least squares estimates $\\hat{\\beta_j}$ are equal to $\\frac{<\\mathbf{x_j},\\mathbf{y}>}{<\\mathbf{x_j},\\mathbf{x_j}>}$—the univariate estimates. In other words, <font color='red'>when the inputs are orthogonal, they have no eﬀect on each other’s\n",
    "parameter estimates in the model.</font>\n",
    "\n",
    "### Orthogonalization\n",
    "\\begin{align}\n",
    "\\hat{\\beta}_1&=\\frac{<\\mathbf{x}-\\bar{x}\\mathbf{1},\\mathbf{y}>}{<\\mathbf{x}-\\bar{x}\\mathbf{1},\\mathbf{x}-\\bar{x}\\mathbf{1}>} \\\\\n",
    "\\end{align}\n",
    "\n",
    "-  $\\bar{x}=\\sum_ix_i/N$;\n",
    "- $\\mathbf{1}$, the vector of N ones;\n",
    "\n",
    "**Steps:**\n",
    "1. regress $\\mathbf{x}$ on $\\mathbf{1}$ to produce the residual $\\mathbf{z}=\\mathbf{x}-\\bar{x}\\mathbf{1}$;\n",
    "2. regress $\\mathbf{y}$ on the residual $\\mathbf{z}$ to give the coeﬃcient $\\hat{\\beta}_1$\n",
    "\n",
    "> regress $\\mathbf{a}$ on $\\mathbf{b}$: ($\\mathbf{b}$ is adjusted for $\\mathbf{a}$),(or $\\mathbf{b}$ is **“orthogonalized”** with respect to  $\\mathbf{a}$); a simple univariate regression of $\\mathbf{b}$ on a with no intercept, producing coeﬃcient $\\hat{\\lambda}=\\frac{<\\mathbf{a},\\mathbf{b}>}{<\\mathbf{a},\\mathbf{a}>}$ and residual vector $ \\mathbf{b}-\\hat{\\lambda} \\mathbf{a}$.\n",
    "\n",
    "The orthogonalization does not change the subspace spanned by x1 and x2, it simply produces an **orthogonal basis** for representing it.\n",
    "\n",
    "\n",
    "### Gram–Schmidt procedure for multiple regression\n",
    "\n",
    "---\n",
    "**ALGORITHM 3.1 Regression by Successive Orthogonalization**\n",
    "1. Initialize $\\mathbf{z_0}=\\mathbf{x_0}=\\mathbf{1}$.\n",
    "2. For j=1,2,...,1,,...,p,<br>\n",
    "  Regress $\\mathbf{x_j}$ on $\\mathbf{z_0},\\mathbf{z_1},...,\\mathbf{z_{j-1}}$ to produce coeﬃcients $\\hat{\\lambda}_{l,j}=\\frac{<\\mathbf{z_l},\\mathbf{x_j}>}{<\\mathbf{z_l},\\mathbf{z_l}>}$, l=0,1,...,j-1, and residual vector $\\mathbf{z_j}=\\mathbf{x_j}-\\sum_{k=0}^{j-1}\\hat{\\lambda_{kj}}\\mathbf{z_k}$\n",
    "3. Regress $\\mathbf{y}$ on the residual $\\mathbf{z_p}$ to give the estimate $\\hat{\\beta_p}=\\frac{<\\mathbf{z_p},\\mathbf{y}>}{<\\mathbf{z_p},\\mathbf{z_p}>}$.\n",
    "---\n",
    "Note:\n",
    "- Each of the $\\mathbf{x}_j$ is a linear combination of the $\\mathbf{z}_k$,k ≤ j.\n",
    "- Since th e$\\mathbf{z}_j$ are all orthogonal, they form a basis for the column space of $\\mathbf{X}$, and hence the least squares projection\n",
    "onto this subspace is $\\mathbf{\\hat{y}}$.\n",
    "- By rearranging the xj , any one of them could be in the last position, and a similar results holds.\n",
    "- The multiple regression coeﬃcient $\\mathbf{x}_j$ represents the additional contribution of $\\mathbf{x}_j$ on $\\mathbf{y}$, after $\\mathbf{x}_j$ has been adjusted for x0, x1,..., xj−1,xj+1,..., xp.\n",
    "\n",
    "### Precision of Coefficient Estimation\n",
    "\n",
    "If $\\mathbf{x}_p$ is highly correlated with some of the other $\\mathbf{x}_k$’s, the residual vector $\\mathbf{z}_p$ will be close to zero, and the coeﬃcient $\\mathbf{x}_j$ will be very unstable.\n",
    "\n",
    "From $\\hat{\\beta_p}=\\frac{<\\mathbf{z_p},\\mathbf{y}>}{<\\mathbf{z_p},\\mathbf{z_p}>}$, we also obtain an alternate formula for the variance estimates:\n",
    "\n",
    "\n",
    "\\begin{align}Var(\\hat{\\beta}_p)=\\frac{\\sigma^2}{<\\mathbf{z_p},\\mathbf{z_p}>}=\\frac{\\sigma^2}{||\\mathbf{z_p}||^2} \\end{align}\n",
    "\n",
    "The precision with which we can estimate $\\hat{\\beta_p}$ depends on the length of the residual vector $\\mathbf{z_p}$; this represents how much of $\\mathbf{x_p}$ is unexplained by the other $\\mathbf{x_k}$’s\n",
    "\n",
    "### QR decomposition\n",
    "We can represent step 2 of Algorithm 3.1 in matrix form:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{X}=\\mathbf{Z}\\mathbf{Γ}\n",
    "\\end{align}\n",
    "\n",
    "- $\\mathbf{Z}$ has as columns the $\\mathbf{z_j}$ (in order)\n",
    "- $\\mathbf{Γ}$ is the upper triangular matrix with entries $\\hat{\\lambda}_{kj}$\n",
    "\n",
    "Introducing the diagonal matrix D with jth diagonal entry $D_{jj} = ||\\mathbf{z_j}||$, we get \n",
    "\n",
    "**QR decomposition of X**:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{X}=\\mathbf{Z}\\mathbf{D}^{-1}\\mathbf{D}\\mathbf{Γ}=\\mathbf{Q}\\mathbf{R}\n",
    "\\end{align}\n",
    "\n",
    "- Q is an N ×(p+1) orthogonal matrix, $Q^TQ=I$;\n",
    "- R is a (p +1) × (p + 1) upper triangular matrix.\n",
    "\n",
    "least squares solution:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta}&=R^{-1}Q^T\\mathbf{y} \\\\\n",
    "\\mathbf{\\hat{y}}&=QQ^T\\mathbf{y}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.4 Multiple Outputs\n",
    "\n",
    "Suppose we have multiple outputs Y1,Y2,...,YK that we wish to predict from our inputs X0,X1,X2,...,Xp. We assume a linear model for each\n",
    "output:\n",
    "\n",
    "\\begin{align}\n",
    "Y_k&=\\beta_{0k}+\\sum_{j=1}^pX_j\\beta_{jk}+\\epsilon_k \\\\\n",
    "&=f_k(X)+\\epsilon_k\n",
    "\\end{align}\n",
    "\n",
    "With N training cases we can write the model in matrix notation:\n",
    "\n",
    "\\begin{align}\n",
    "Y=XB+E\n",
    "\\end{align}\n",
    "\n",
    "- Y: N×K response matrix\n",
    "- X: N×(p+1) input matrix\n",
    "- B: (p+1)× K matrix of parameters\n",
    "- E: N×K matrix of errors\n",
    "\n",
    "A straightforward generalization of the univariate loss function:\n",
    "\n",
    "\\begin{align}\n",
    "RSS(B)&=\\sum_{k=1}^K\\sum_{i=1}^N(y_{ik}-f_k(x_i))^2 \\\\\n",
    "&=tr[(Y-XB)^T(Y-XB)]\n",
    "\\end{align}\n",
    "\n",
    "The least squares estimates have exactly the same form as before:\n",
    "\\begin{align}\n",
    "\\hat{B}=(X^TX)^{-1}X^Ty\n",
    "\\end{align}\n",
    "\n",
    "If the errors $\\epsilon =(\\epsilon_1,...,\\epsilon_K)$ in are correlated, suppose $Cov(\\epsilon)= Σ$, then the multivariate weighted criterion:\n",
    "\\begin{align}\n",
    "RSS(B;Σ)&=\\sum_{i=1}^N(y_{ik}-f_k(x_i))^TΣ^{-1}(y_{ik}-f_k(x_i)) \n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
