{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiple\n",
    "linear regression model takes the form:\n",
    "\n",
    "\\begin{align}\n",
    "Y=\\beta_0+\\beta_1X_1+,,,+\\beta_pX_p+\\epsilon\n",
    "\\end{align}\n",
    "\n",
    "# Estimating the Regression Coefficients \n",
    "\n",
    "We choose β0, β1, . . . , βp\n",
    "to minimize the sum of squared residuals\n",
    "\n",
    "\\begin{align}\n",
    "RSS&=\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\\\\n",
    "&=\\sum_{i=1}^n(y_i-\\hat{\\beta_0}-\\hat{\\beta_1}x_{i1}-,,,-\\hat{\\beta_p}x_{ip})^2\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"./images/5.png\" width=600>\n",
    "\n",
    "**Does it make sense for the multiple regression to suggest no relationship\n",
    "between *sales* and *newspaper* while the simple linear regression implies the\n",
    "opposite?**\n",
    "\n",
    "- Notice that the correlation between radio and newspaper is 0.35.\n",
    "- In markets where we spend more\n",
    "on radio our sales will tend to be higher, and as our correlation matrix\n",
    "shows, we also tend to spend more on newspaper advertising in those same\n",
    "markets. \n",
    "- Hence, in a simple linear regression which only examines sales\n",
    "versus newspaper, we will observe that higher values of newspaper tend to be\n",
    "associated with higher values of sales, even though newspaper advertising\n",
    "does not actually affect sales. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Important Questions\n",
    "\n",
    "## Is There a Relationship Between the Response and Predictors?\n",
    "\n",
    "**Hypothesis Test**\n",
    "\n",
    "We use a hypothesis test to answer this question. \n",
    "\n",
    "We test the **null hypothesis**\n",
    "\n",
    "```\n",
    "H_0 : β1 = β2 = · · · = βp = 0\n",
    "```\n",
    "versus the **alternative**\n",
    "```\n",
    "H_a : at least one βj is non-zero\n",
    "```\n",
    "\n",
    "This hypothesis test is performed by computing the **F-statistic**,\n",
    "\n",
    "\\begin{align}\n",
    "F=\\frac{(TSS-RSS)/p}{RSS/(n-p-1)}\n",
    "\\end{align}\n",
    "\n",
    "where $TSS =(y_i − \\bar{y})^2$ and $RSS =(y_i−\\hat{y}_i)^2$. \n",
    "\n",
    "If the linear model assumptions are correct, one can show that\n",
    "\n",
    "\\begin{align}\n",
    "E[RSS/(n-p-1)]=\\sigma^2\n",
    "\\end{align}\n",
    "\n",
    "and that, provided H0 is true,\n",
    "\n",
    "\\begin{align}\n",
    "E[(TSS-RSS)/p]=\\sigma^2\n",
    "\\end{align}\n",
    "\n",
    "- When there is no relationship between the response and predictors,\n",
    "one would expect the F-statistic to take on a value close to 1. \n",
    "\n",
    "- On the other hand, if H_a is true, then $E[(TSS-RSS)/p]>\\sigma^2$, so we expect F to be\n",
    "greater than 1.\n",
    "\n",
    "<img src=\"./images/6.png\" width=600>\n",
    "\n",
    "**How large does the F-statistic need to be before we can reject H0 and\n",
    "conclude that there is a relationship?**\n",
    "\n",
    "- When n is large, an F-statistic that is just a\n",
    "little larger than 1 might still provide evidence against H_0. \n",
    "- In contrast, a larger F-statistic is needed to reject H_0 if n is small.\n",
    "- For the advertising data, the **p-value** associated with\n",
    "the F-statistic in Table 3.6 is essentially zero, so we have extremely strong\n",
    "evidence that at least one of the media is associated with increased sales.\n",
    "\n",
    "**To test that a particular subset of q of the coefficients are zero**\n",
    "\n",
    "This corresponds to a null hypothesis\n",
    "\n",
    "```\n",
    "H_0 : β(p-q+1) = β(p-q+2) = · · · = βp = 0\n",
    "```\n",
    "\n",
    "In this case we fit a second model that uses all the variables\n",
    "**except those last q**. Suppose that the residual sum of squares for that model\n",
    "is RSS_0. Then the appropriate F-statistic is\n",
    "\n",
    "\\begin{align}\n",
    "F=\\frac{(RSS_0-RSS)/q}{RSS/(n-p-1)}\n",
    "\\end{align}\n",
    "\n",
    "**F-statistics v.s. t-statistics**\n",
    "\n",
    "- **Equivalency**: In Table 3.4, for each individual predictor a t-statistic and\n",
    "a p-value were reported. These provide information about whether each\n",
    "individual predictor is related to the response, after adjusting for the other\n",
    "predictors. It turns out that each of these are exactly equivalent  to the\n",
    "F-test that omits that single variable from the model, leaving all the others\n",
    "in—i.e. q=1 in the model. So it reports the **partial effect** of adding that variable\n",
    "to the model.\n",
    "\n",
    "> The square of each *t-statistic* is the corresponding *F-statistic*.\n",
    "\n",
    "- **p is large**: If any one of the\n",
    "p-values for the individual variables is very small, then ***at least one of the\n",
    "predictors is related to the response***. However, this logic is flawed, especially\n",
    "when the number of predictors p is large.\n",
    " - If we use the individual t-statistics and associated p-values to decide whether there is any association between\n",
    "the variables and the response, high chance we will\n",
    "incorrectly conclude there is a relationship. \n",
    " - However, the F-statistic\n",
    "does not suffer from this problem because it adjusts for the number of\n",
    "predictors.\n",
    "\n",
    "- **p > n**: more coefficients βj to estimate than observations from which to estimate them. \n",
    " - cannot\n",
    "even fit the multiple linear regression model using least squares,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do all the predictors help to explain Y , or is only a subset of the predictors useful? \n",
    "\n",
    "**Variable Selection**\n",
    "\n",
    "- Various statistics can be used to\n",
    "judge the quality of a model:\n",
    " - **Mallow’s Cp, Akaike informa-\n",
    "Mallow’s Cp tion criterion (AIC)**\n",
    " - **Bayesian information criterion (BIC)**\n",
    " - **adjusted R2**\n",
    " \n",
    "- There\n",
    "are three classical approaches to select models:\n",
    " - **Forward selection**\n",
    " - **Backward selection**\n",
    " - **Mixed selection**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How well does the model fit the data?\n",
    "\n",
    "Two of the most common numerical measures of model fit are the **RSE** and\n",
    "**R2**\n",
    "\n",
    "### R2 Statistics\n",
    "\n",
    "An $R^2$ value close to 1 indicates that the model explains a large portion\n",
    "of the variance in the response variable.\n",
    "\\begin{align}\n",
    "R2 = (TSS − RSS)/TSS= 1− RSS/TSS\n",
    "\\end{align}\n",
    "\n",
    "Recall that in simple regression, R2 is the square of the correlation of the\n",
    "response and the variable. In multiple linear regression, it turns out that it\n",
    "equals $Cor(Y, \\hat{Y} )^2$, the square of the correlation between the response and\n",
    "the fitted linear model; in fact one property of the fitted linear model is\n",
    "that it maximizes this correlation among all possible linear models.\n",
    "\n",
    "\n",
    "**R2 will always increase when more variables are added to the model, even if those variables are only weakly associated\n",
    "with the response.**\n",
    "- This is due to the fact that adding another variable to\n",
    "the least squares equations must allow us to fit the training data (though\n",
    "not necessarily the testing data) more accurately.\n",
    "- The fact that\n",
    "adding *newspaper* advertising to the model containing only TV and radio\n",
    "advertising leads to just a tiny increase in R2 provides additional evidence\n",
    "that newspaper can be dropped from the model.\n",
    "\n",
    "### RSE\n",
    "\n",
    "**RSE** is defined as\n",
    "\n",
    "\\begin{align}\n",
    "RSE=\\sqrt{\\frac{RSS}{n-p-1}}\n",
    "\\end{align}\n",
    "\n",
    "Models with\n",
    "more variables can have higher RSE if the decrease in RSS is small relative\n",
    "to the increase in p.\n",
    "\n",
    "### Graphical summaries\n",
    "\n",
    "<img src=\"./images/7.png\" width=600>\n",
    "It suggests a **synergy** or **interaction** effect between the advertising\n",
    "media, whereby combining the media together results in a bigger boost to\n",
    "sales than using any single medium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a set of predictor values, what response value should we predict, and how accurate is our prediction? \n",
    "\n",
    "**Uncertainty associated with prediction**\n",
    "\n",
    "1. The coefficient estimates $\\hat{\\beta_0},\\hat{\\beta_1},...,\\hat{\\beta_p}$ are estimates for $β_0, β_1, . . . , β_p$. That is, the **least squares plane**\n",
    "\\begin{align}\n",
    "\\hat{Y}=\\hat{\\beta_0}+\\hat{\\beta_1}X_1+,...+\\hat{\\beta_p}X_p\n",
    "\\end{align}\n",
    "is only an estimate for the **true population regression plane**\n",
    "\\begin{align}\n",
    "f(X)=\\beta_0+\\beta_1X_1+,...+\\beta_pX_p\n",
    "\\end{align}\n",
    " - The inaccuracy in the coefficient estimates is related to the **reducible\n",
    "error**. \n",
    "\n",
    " - We can compute a **confidence interval** in order\n",
    "to determine how close $\\hat{Y}$ will be to f(X).\n",
    "\n",
    "2. In practice assuming a linear model for f(X) is almost\n",
    "always an approximation of reality, so there is an additional source of\n",
    "potentially **reducible error** which we call **model bias**.\n",
    "\n",
    "3. Even if we knew f(X)—true values for β0, β1, . . . , βp—the response value cannot be predicted perfectly\n",
    "because of the random error $\\epsilon$ --**irreducible error**. \n",
    " - How much will Y vary from $\\hat{Y}$? -- **prediction intervals** \n",
    "\n",
    "### **Prediction intervals** \n",
    "\n",
    "**Prediction intervals** are always wider than **confidence intervals**\n",
    " - Because they incorporate both *the error in the estimate for f(X) (the reducible\n",
    "error) and the uncertainty as to how much an individual point will\n",
    "differ from the population regression plane (the irreducible error).*\n",
    "\n",
    "E.g. \n",
    "- **confidence interval** : quantify the uncertainty surrounding the average sales over a large number of cities.\n",
    "- **prediction interval** : quantify the uncertainty surrounding sales for a particular city."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
