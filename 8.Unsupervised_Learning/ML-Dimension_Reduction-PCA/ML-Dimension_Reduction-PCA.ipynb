{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Study Note: Dimension Reduction - PCA, PCR\"\n",
    "date: '2019–06–14 00:00:35'\n",
    "categories: Machine Learning\n",
    "mathjax: true\n",
    "abbrlink: cac93a23\n",
    "tags: \n",
    "- PCA\n",
    "- Dimension Reduction\n",
    "- Model Selection\n",
    "comments: true\n",
    "---\n",
    "\n",
    "# Dimension Reduction Methods\n",
    "\n",
    "Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp. \n",
    "\n",
    "Dimension Reduction Methods ***transform*** the predictors and then fit a least\n",
    "squares model using the transformed variables.\n",
    "\n",
    "## Approach\n",
    "Let $Z_1,Z_2, . . . ,Z_M$ represent $M < p$ linear combinations of our original $p$ predictors. That is,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z_m=\\sum_{j=1}^p\\phi_{jm}X_j\n",
    "\\end{align}\n",
    "$$\n",
    "<!--more-->\n",
    "\n",
    "for some constants $φ_{1m}, φ_{2m} . . . , φ_{pm}, m = 1, . . .,M.$ We can then fit the\n",
    "linear regression model\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i=\\theta_0+\\sum_{m=1}^M\\theta_m z_{im}+\\epsilon_i  \\quad  i=1,2,3,4,...,n\n",
    "\\end{align}\n",
    "$$\n",
    "**Dimension reduction**: reduces the problem of estimating the $p+1$ coefficients $β_0, β_1, . . . , β_p$ to the\n",
    "simpler problem of estimating the $M + 1$ coefficients $θ_0, θ_1, . . . , θ_M$, where\n",
    "M < p. In other words, the dimension of the problem has been reduced\n",
    "from $p + 1$ to $M + 1$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{m=1}^M\\theta_m z_{im}&=\\sum_{m=1}^M\\theta_m \\sum_{j=1}^p\\phi_{jm}x_{ij}=\\sum_{m=1}^M\\sum_{j=1}^p\\theta_m \\phi_{jm}x_{ij}=\\sum_{j=1}^p \\beta_jx_{ij}  \\\\\n",
    "\\beta_j&=\\sum_{m=1}^M\\theta_m \\phi_{jm}\n",
    "\\end{align}\n",
    "$$\n",
    "**All dimension reduction methods work in two steps:**\n",
    "\n",
    "1. The transformed predictors $Z_1,Z_2, . . . ,Z_M$are obtained.\n",
    "2. The model is fit using these $M$ predictors. However, the choice of $Z_1,Z_2, . . . ,Z_M$, or equivalently,\n",
    "the selection of the $φ_{jm}$’s, can be achieved in different ways. \n",
    "\n",
    "# Principal Components Regression \n",
    "\n",
    "\n",
    "## An Overview of Principal Components Analysis\n",
    "\n",
    "**Principal component analysis (PCA)** refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data.\n",
    "\n",
    "- PCA also serves as a tool for data visualization (visualization of\n",
    "  the observations or visualization of the variables).\n",
    "\n",
    "## What Are Principal Components?\n",
    "\n",
    "**PCA** :finds a low-dimensional representation of a data set that contains as much as possible of the **variation**\n",
    "\n",
    "Each of the dimensions found by PCA is a linear combination of the $p$ features. \n",
    "\n",
    "***The first principal component*** of a set of features $X_1,X_2, . . . , X_p$ is the normalized linear combination of the features\n",
    "$$\n",
    "\\begin{align}\n",
    "Z_1=\\phi_{11}X_1+\\phi_{21}X_2+,,,+\\phi_{p1}X_p\n",
    "\\end{align}\n",
    "$$\n",
    " that has the **largest variance**. \n",
    "\n",
    "**Normalized**: $\\sum_{j=1}^p \\phi_{j1}^2=1$\n",
    "\n",
    "**Loadings**: $\\phi_{11}, . . . , \\phi_{p1}$ the loadings of the first principal component; \n",
    "\n",
    "- Together, the loadings make up the principal component loading vector, $\\phi_1=(\\phi_{11},\\phi_{21},...,\\phi_{p1})^T$\n",
    "\n",
    "\n",
    "\n",
    "### 1st Principal Component\n",
    "\n",
    "#### Interpretation 1: greatest variability\n",
    "\n",
    "**The first principal component** direction of the data: is that along which the observations **vary the most**.\n",
    "\n",
    "<img src=\"./7.png\" width=\"600\" />\n",
    "\n",
    "The first principal component direction is the direction along which there is the greatest variability in the data. That is, if we projected the 100 observations onto this line (as shown in the left-hand panel of Figure 6.15), then the resulting projected observations would have the largest possible variance\n",
    "\n",
    "<img src=\"./8.png\" width=\"600\" />\n",
    "\n",
    "The first principal component is given by the formula\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z_1 = 0.839 × (pop − \\bar{pop}) + 0.544 × (ad − \\bar{ad})\n",
    "\\end{align}\n",
    "$$\n",
    "Here $φ_{11} = 0.839$ and $φ_{21} = 0.544$ are the **principal component loadings**,\n",
    "which define the direction referred to above.\n",
    "\n",
    ">The idea is that out of every possible linear combination\n",
    "of pop and ad such that $\\phi_{11}^2+\\phi_{21}^2=1$, this particular linear combination\n",
    "yields the highest variance: i.e. this is the linear combination for which\n",
    "$Var(φ_{11} × (pop − \\bar{pop}) + φ_{21} × (ad − \\bar{ad}))$ is maximized.\n",
    "\n",
    "**Principal Component Scores**\n",
    "\n",
    "The values of $z_{11}, . . . , z_{n1}$ are known as the **principal component scores**, and\n",
    "can be seen in the right-hand panel of Figure 6.15. For example,\n",
    "$$\n",
    "\\begin{align}\n",
    "z_{i1} = 0.839 × (pop_i − \\bar{pop}) + 0.544 × (ad_i − \\bar{ad})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Interpretation 2: closest to data\n",
    "\n",
    "\n",
    "There is also another interpretation for PCA: the first principal component\n",
    "vector defines the line that is as close as possible to the data. \n",
    "\n",
    "In Figure 6.14, the first principal component line minimizes the\n",
    "sum of the squared perpendicular distances between each point and the\n",
    "line.\n",
    "\n",
    "In the right-hand panel of Figure 6.15, the left-hand panel has been\n",
    "rotated so that the first principal component direction coincides with the\n",
    "x-axis. It is possible to show that the ***first principal component score*** for\n",
    "the ith observation is the distance in the $x$-direction of the\n",
    "ith cross from zero.\n",
    "\n",
    "#### Interpretation 3: single number summarization\n",
    "\n",
    "We can think of the values of the principal component $Z_1$ as single number\n",
    "summaries of the joint pop and ad budgets for each location. \n",
    "\n",
    "In this example, if $z_{i1} = 0.839 × (pop_i − pop) + 0.544 × (ad_i − ad) < 0$, then this indicates a city with below-average population size and belowaverage ad spending.\n",
    "\n",
    "<img src=\"./9.png\" width=\"650\" />\n",
    "\n",
    "Figure 6.16 displays\n",
    "$z_{i1}$ versus both pop and ad. The plots show a strong relationship between\n",
    "the first principal component and the two features. In other words, the first\n",
    "principal component appears to *capture most of the information* contained\n",
    "in the pop and ad predictors.\n",
    "\n",
    "\n",
    "\n",
    "#### Compute the first principal component\n",
    "\n",
    "- Assume that each of the variables in $X$ has been centered to have mean zero. We then look for the linear combination of the sample feature values of the form\n",
    "  $$\n",
    "  \\begin{align}\n",
    "  z_{i1}=\\phi_{11}x_{i1}+\\phi_{21}x_{i2}+,,,+\\phi_{p1}x_{ip} \\quad \\quad i=1,2,...,n\n",
    "  \\end{align}\n",
    "  $$\n",
    "  that has largest sample variance, subject to the constraint that $\\sum_{j=1}^p \\phi_{j1}^2=1$\n",
    "\n",
    "- The first principal component loading vector solves the optimization problem\n",
    "  $$\n",
    "  \\begin{align}\n",
    "  \\max_{\\phi_{11},...,\\phi_{p1}}{\\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\left( \\sum_{j=1}^p \\phi_{j1}x_{ij}   \\right)^2 \\right\\}} \\, subject \\, to \\, \\sum_{j=1}^p \\phi_{j1}^2=1\n",
    "  \\end{align}\n",
    "  $$\n",
    "\n",
    "- Since $\\sum_{i=1}^nx_{ij}/n=1$, the average of the $z_{11}, . . . , z_{n1}$ will be zero as well. Hence\n",
    "  the objective that we are maximizing is just the **sample variance** of the $n$ values of zi1\n",
    "\n",
    "- **Scores**: We refer to $z_{11}, . . . , z_{n1}$ as the scores of the first principal component.\n",
    "\n",
    "**Geometric interpretation**: for the first principal component: The loading vector $\\phi_1$ with elements $\\phi_{11},\\phi_{21},...,\\phi_{p1}$ defines a direction in\n",
    "feature space along which the data **vary the most**. If we project the n data\n",
    "points $x_1, . . . , x_n$ onto this direction, the projected values are the principal\n",
    "component scores $z_{11}, . . . , z_{n1}$ themselves.\n",
    "\n",
    "\n",
    "\n",
    "### 2nd Principal Component\n",
    "\n",
    "The s**econd principal component $Z_2$** is a linear combination of the variables that is uncorrelated\n",
    "with $Z_1$, and has largest variance subject to this constraint.\n",
    "\n",
    "It turns out that the zero correlation condition of $Z_1$  with $Z_2$  is equivalent to the condition that the direction must be perpendicular, or orthogonal, to the first principal component direction. \n",
    "\n",
    "The second principal component is given by the formula:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z_2 = 0.544 × (pop − \\bar{pop}) − 0.839 × (ad − \\bar{ad}).\n",
    "\\end{align}\n",
    "$$\n",
    "Figure 6.15. The fact that the second principal component scores are much closer to zero indicates that\n",
    "this component captures far less information.\n",
    "\n",
    "\n",
    "\n",
    "#### Compute the second principal component\n",
    "\n",
    "**The second principal component $Z_2$**: the linear combination of $X_1,X_2, . . . , X_p$ that has maximal\n",
    "variance out of all linear combinations that are **uncorrelated with $Z_1$**. \n",
    "\n",
    "The second principal component scores $z_{12}, . . . , z_{n2}$ take the form \n",
    "$$\n",
    "\\begin{align}\n",
    "z_{i2}=\\phi_{12}x_{i1}+\\phi_{22}x_{i2}+,,,+\\phi_{p2}x_{ip} \\quad \\quad i=1,2,...,n\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\phi_2$ is the second principal component **loading** vector, with elements\n",
    "$\\phi_{12},\\phi_{22},...,\\phi_{p2}$.\n",
    "\n",
    "It turns out that constraining $Z_2$ to be uncorrelated with $Z_1$ is equivalent to constraining the direction $\\phi_2$ to be **orthogonal** (perpendicular) to the direction $\\phi_1$.\n",
    "\n",
    "To find $\\phi_2$, we solve a problem similar to (10.3) with $\\phi_2$ replacing $\\phi_1$, and with the additional constraint that $\\phi_2$ is orthogonal to $\\phi_1$\n",
    "\n",
    "<img src=\"./1_v2.png\" width=\"600\" /> \n",
    "\n",
    "**Interpretation:** \n",
    "\n",
    "- 1st loading vector places approximately equal weight on Assault, Murder, and Rape, with much less weight UrbanPop. Hence this component roughly corresponds to a measure of overall\n",
    "  rates of serious crimes.\n",
    "- Overall, we see that the crime-related variables (Murder, Assault, and Rape)\n",
    "  are located close to each other, and that the UrbanPop variable is far from\n",
    "  the other three.\n",
    "- This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high\n",
    "  assault and rape rates—and that the UrbanPop variable is less correlated\n",
    "  with the other three.\n",
    "\n",
    "\n",
    "\n",
    "## Another Interpretation of Principal Components \n",
    "\n",
    "**An alternative interpretation for principal components**: principal components provide low-dimensional linear surfaces that are closest to the observations\n",
    "\n",
    "- **The first principal component loading vector has a very special property**:\n",
    "  it is the line in p-dimensional space that is closest to the n observations\n",
    "  (using average squared Euclidean distance as a measure of closeness).\n",
    "\n",
    "- The appeal of this interpretation : we seek a single dimension of the data that lies as close as possible to all of the data points, since such a line will likely provide a good summary of the\n",
    "  data.\n",
    "\n",
    "- **The first two principal components** of a data set **span the plane** that is closest to the n observations, in terms of average squared Euclidean distance\n",
    "\n",
    "- Together **the first M principal component** score vectors and the first M principal component loading vectors provide the best M-dimensional approximation (in terms of Euclidean distance) to the ith observation $x_{ij}$ .\n",
    "  $$\n",
    "  \\begin{align}\n",
    "  x_{ij} \\approx \\sum_{m=1}^Mz_{im}\\phi_{jm}\n",
    "  \\end{align}\n",
    "  $$\n",
    "  (assuming the original data matrix X is column-centered).\n",
    "\n",
    "- When $M = min(n − 1, p)$, then the representation is exact: $x_{ij} = \\sum_{m=1}^Mz_{im}\\phi_{jm}$\n",
    "\n",
    "\n",
    "\n",
    "## More on PCA\n",
    "\n",
    "### Scaling the Variables\n",
    "\n",
    "Before PCA is performed, the variables should be **centered to have mean zero**. Furthermore, the results obtained when we perform PCA will also depend on whether the variables have been\n",
    "**individually scaled** (each multiplied by a different constant)\n",
    "\n",
    "<img src=\"./2_v2.png\" width=\"600\" /> \n",
    "\n",
    "### Uniqueness of the Principal Components \n",
    "\n",
    "**Each principal component loading vector $\\phi_1=(\\phi_{11},\\phi_{21},...,\\phi_{p1})^T$ and the score vectors $z_{11}, . . . , z_{n1}$ is unique, up to a sign flip. **\n",
    "\n",
    "- Two different software packages will yield the same principal\n",
    "  component loading vectors and score vectors, although the signs of those loading vectors\n",
    "  may differ. \n",
    "- **The signs may differ** because each principal component loading\n",
    "  vector specifies a direction in p-dimensional space: flipping the sign has no\n",
    "  effect as the direction does not change.\n",
    "\n",
    "\n",
    "\n",
    "### The Proportion of Variance Explained \n",
    "\n",
    "**How much of the variance in the data is not contained in the first few principal components?** \n",
    "\n",
    "**Proportion of variance explained (PVE)** by each principal component: \n",
    "\n",
    "- The total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{j=1}^pVar(X_j)=\\sum_{j=1}^p\\frac{1}{n}\\sum_{i=1}^nx_{ij}^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- The variance explained by the mth principal component is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{1}{n}\\sum_{i=1}^nz_{im}^2=\\frac{1}{n}\\sum_{i=1}^n \\left( \\sum_{j=1}^p \\phi_{jm}x_{ij} \\right)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- Therefore, the **PVE of the mth principal component** is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\sum_{i=1}^n \\left( \\sum_{j=1}^p \\phi_{jm}x_{ij} \\right)^2}{\\sum_{j=1}^p\\sum_{i=1}^nx_{ij}^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The PVE of each principal component is a positive quantity. In order to\n",
    "compute the **cumulative PVE** of the first $M$ principal components, we\n",
    "can simply sum (10.8) over each of the first $M$ PVEs. In total, there are\n",
    "$min(n − 1, p)$ principal components, and their PVEs sum to one.\n",
    "\n",
    "<img src=\"./3_v2.png\" width=\"600\" /> \n",
    "\n",
    "### Deciding How Many Principal Components to Use\n",
    "\n",
    "We would like to use the smallest number of principal components required to get a good understanding of the data. \n",
    "\n",
    "**How many principal components are needed?**\n",
    "\n",
    "- We typically decide on the number of principal components required to visualize the data by examining a **scree plot** (Right FIGURE 10.4)\n",
    "- We choose the smallest number of\n",
    "  principal components that are required in order to explain a sizable amount\n",
    "  of the variation in the data.\n",
    "- We tend to look\n",
    "  at the first few principal components in order to find interesting patterns\n",
    "  in the data. If no interesting patterns are found in the first few principal\n",
    "  components, then further principal components are unlikely to be of interest.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The Principal Components Regression Approach\n",
    "\n",
    "The principal components regression (PCR) approach involves constructing the first M principal components, $Z_1,Z_2, . . . ,Z_M$, and then using these components\n",
    "as the predictors in a linear regression model that is fit\n",
    "using least squares\n",
    "\n",
    "\n",
    "**The key idea**\n",
    "\n",
    "Often a small number of principal components suffice to explain most of the variability in the data, as\n",
    "well as the relationship with the response. In other words, we assume that\n",
    "***the directions in which $X_1, . . .,X_p$ show the most variation are the directions\n",
    "that are associated with $Y$***\n",
    "\n",
    "**Example**:\n",
    "\n",
    "<img src=\"./10.png\" width=\"650\" />\n",
    "\n",
    "- Performing PCR with an appropriate\n",
    "choice of M can result in a substantial improvement over least squares\n",
    "- PCR does not perform as well as the two shrinkage methods\n",
    "   - **Reason**: The data were generated in such a way that many principal\n",
    "  components are required in order to adequately model the response.\n",
    "  In contrast, PCR will tend to do well in cases when the first few principal\n",
    "  components are sufficient to capture most of the variation in the predictors\n",
    "  as well as the relationship with the response.\n",
    "\n",
    "**Note**: even though PCR provides a simple way to perform\n",
    "regression using $M < p$ predictors, it is not a *feature selection* method!\n",
    "\n",
    " - This is because each of the $M$ principal components used in the regression is a linear combination of all p of the original features.\n",
    " - PCR is more closely related to ridge regression than\n",
    "to the lasso. One can even think of ridge regression as a continuous version\n",
    "of PCR!\n",
    "\n",
    "**Cross-validation**: In PCR, the number of principal components, $M$, is typically chosen by\n",
    "cross-validation.\n",
    "\n",
    "<img src=\"./11.png\" width=\"650\" />\n",
    "\n",
    "**Standardisation**: When performing PCR, we generally recommend standardizing each\n",
    "predictor, prior to generating the principal components.\n",
    " - In the\n",
    "absence of standardization, the *high-variance variables* will tend to play a\n",
    "larger role in the principal components obtained, and the scale on which\n",
    "the variables are measured will ultimately have an effect on the final PCR\n",
    "model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "**Ref:**\n",
    "\n",
    "James, Gareth, et al. *An introduction to statistical learning*. Vol. 112. New York: springer, 2013.\n",
    "\n",
    "Hastie, Trevor, et al. \"The elements of statistical learning: data mining, inference and prediction.\" *The Mathematical Intelligencer* 27.2 (2005): 83-85"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
