{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis\n",
    "\n",
    "**Principal component analysis (PCA)** refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data.\n",
    "- PCA also serves as a tool for data visualization (visualization of\n",
    "the observations or visualization of the variables).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Principal Components?\n",
    "\n",
    "**PCA** :finds a low-dimensional representation of a data set that contains as much as possible of the **variation**\n",
    "\n",
    "Each of the dimensions found by PCA is a linear combination\n",
    "of the $p$ features. \n",
    "\n",
    "\n",
    "***The first principal component*** of a set of features $X_1,X_2, . . . , X_p$ is the normalized linear combination of the features\n",
    "\n",
    "\\begin{align}\n",
    "Z_1=\\phi_{11}X_1+\\phi_{21}X_2+,,,+\\phi_{p1}X_p\n",
    "\\end{align}\n",
    " that has the **largest variance**. \n",
    "\n",
    "**Normalized**: $\\sum_{j=1}^p \\phi_{j1}^2=1$\n",
    "\n",
    "**Loadings**: $\\phi_{11}, . . . , \\phi_{p1}$ the loadings of the first principal component; \n",
    "- Together, the loadings make up the principal component loading vector, $\\phi_1=(\\phi_{11},\\phi_{21},...,\\phi_{p1})^T$\n",
    "\n",
    "### Compute the first principal component\n",
    "- Assume that each of\n",
    "the variables in $X$ has been centered to have mean zero. We then look for the linear combination of the\n",
    "sample feature values of the form\n",
    "\\begin{align}\n",
    "z_{i1}=\\phi_{11}x_{i1}+\\phi_{21}x_{i2}+,,,+\\phi_{p1}x_{ip} \\quad \\quad i=1,2,...,n\n",
    "\\end{align}\n",
    "that has largest sample variance, subject to the constraint that $\\sum_{j=1}^p \\phi_{j1}^2=1$\n",
    "\n",
    "- The first principal component loading vector solves the optimization problem\n",
    "\\begin{align}\n",
    "\\max_{\\phi_{11},...,\\phi_{p1}}{\\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\left( \\sum_{j=1}^p \\phi_{j1}x_{ij}   \\right) \\right\\}} \\, subject \\, to \\, \\sum_{j=1}^p \\phi_{j1}^2=1\n",
    "\\end{align}\n",
    " - Since $\\sum_{i=1}^nx_{ij}/n=1$, the average of the $z_{11}, . . . , z_{n1}$ will be zero as well. Hence\n",
    "the objective that we are maximizing is just the **sample variance** of\n",
    "the $n$ values of zi1\n",
    " - **Scores**: We refer to $z_{11}, . . . , z_{n1}$ as the scores of the first principal component.\n",
    "\n",
    "**Geometric interpretation**: for the first principal component: The loading vector $\\phi_1$ with elements $\\phi_{11},\\phi_{21},...,\\phi_{p1}$ defines a direction in\n",
    "feature space along which the data **vary the most**. If we project the n data\n",
    "points $x_1, . . . , x_n$ onto this direction, the projected values are the principal\n",
    "component scores $z_{11}, . . . , z_{n1}$ themselves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the second principal component\n",
    "\n",
    "**The second principal component $Z_2$**: the linear combination of $X_1,X_2, . . . , X_p$ that has maximal\n",
    "variance out of all linear combinations that are **uncorrelated with $Z_1$**. \n",
    "\n",
    "The\n",
    "second principal component scores $z_{12}, . . . , z_{n2}$ take the form \n",
    "\n",
    "\\begin{align}\n",
    "z_{i2}=\\phi_{12}x_{i1}+\\phi_{22}x_{i2}+,,,+\\phi_{p2}x_{ip} \\quad \\quad i=1,2,...,n\n",
    "\\end{align}\n",
    "where $\\phi_2$ is the second principal component **loading** vector, with elements\n",
    "$\\phi_{12},\\phi_{22},...,\\phi_{p2}$.\n",
    "\n",
    "It turns out that constraining $Z_2$ to be uncorrelated with\n",
    "$Z_1$ is equivalent to constraining the direction $\\phi_2$ to be **orthogonal** (perpendicular)\n",
    "to the direction $\\phi_1$.\n",
    "\n",
    "To find $\\phi_2$, we solve\n",
    "a problem similar to (10.3) with $\\phi_2$ replacing $\\phi_1$, and with the additional\n",
    "constraint that $\\phi_2$ is orthogonal to $\\phi_1$\n",
    "\n",
    "<img src=\"./images/1.png\" width=\"600\"> \n",
    "\n",
    "**Interpretation:** \n",
    "- 1st loading vector places approximately equal weight on Assault, Murder, and Rape, with much less weight UrbanPop. Hence this component roughly corresponds to a measure of overall\n",
    "rates of serious crimes.\n",
    "- Overall, we see that the crime-related variables (Murder, Assault, and Rape)\n",
    "are located close to each other, and that the UrbanPop variable is far from\n",
    "the other three.\n",
    "- This indicates that the crime-related variables are correlated\n",
    "with each other—states with high murder rates tend to have high\n",
    "assault and rape rates—and that the UrbanPop variable is less correlated\n",
    "with the other three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Interpretation of Principal Components \n",
    "\n",
    "**An alternative interpretation for principal components**: principal components provide low-dimensional linear surfaces that are closest to the observations\n",
    "\n",
    "- **The first principal component loading vector has a very special property**:\n",
    "it is the line in p-dimensional space that is closest to the n observations\n",
    "(using average squared Euclidean distance as a measure of closeness).\n",
    " - The appeal of this interpretation : we\n",
    "seek a single dimension of the data that lies as close as possible to all of\n",
    "the data points, since such a line will likely provide a good summary of the\n",
    "data.\n",
    "\n",
    "- **The first two principal components** of a data set\n",
    "**span the plane** that is closest to the n observations, in terms of average\n",
    "squared Euclidean distance\n",
    "\n",
    "- Together **the first M principal component** score\n",
    "vectors and the first M principal component loading vectors provide the\n",
    "best M-dimensional approximation (in terms of Euclidean distance) to\n",
    "the ith observation $x_{ij}$ .\n",
    "\\begin{align}\n",
    "x_{ij} \\approx \\sum_{m=1}^Mz_{im}\\phi_{jm}\n",
    "\\end{align}\n",
    "(assuming the original data matrix X is column-centered).\n",
    "- When $M = min(n − 1, p)$, then the representation\n",
    "is exact: $x_{ij} = \\sum_{m=1}^Mz_{im}\\phi_{jm}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on PCA\n",
    "### Scaling the Variables\n",
    "\n",
    "Before PCA is performed, the variables should be **centered to have mean zero**. Furthermore, the results obtained\n",
    "when we perform PCA will also depend on whether the variables have been\n",
    "**individually scaled** (each multiplied by a different constant)\n",
    "\n",
    "<img src=\"./images/2.png\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniqueness of the Principal Components \n",
    "\n",
    "**Each principal component loading vector $\\phi_1=(\\phi_{11},\\phi_{21},...,\\phi_{p1})^T$ and the score vectors $z_{11}, . . . , z_{n1}$ is unique, up to a sign flip. **\n",
    "- Two different software packages will yield the same principal\n",
    "component loading vectors and score vectors, although the signs of those loading vectors\n",
    "may differ. \n",
    "- **The signs may differ** because each principal component loading\n",
    "vector specifies a direction in p-dimensional space: flipping the sign has no\n",
    "effect as the direction does not change.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Proportion of Variance Explained \n",
    "\n",
    "**How much of the variance in the data is not\n",
    "contained in the first few principal components?** \n",
    "\n",
    "**Proportion of variance explained (PVE)** by each\n",
    "principal component: \n",
    "- The total variance present in a data set (assuming\n",
    "that the variables have been centered to have mean zero) is defined as\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{j=1}^pVar(X_j)=\\sum_{j=1}^p\\frac{1}{n}\\sum_{i=1}^nx_{ij}^2\n",
    "\\end{align}\n",
    "\n",
    "- The variance explained by the mth principal component is\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{n}\\sum_{i=1}^nz_{im}^2=\\frac{1}{n}\\sum_{i=1}^n \\left( \\sum_{j=1}^p \\phi_{jm}x_{ij} \\right)^2\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "- Therefore, the **PVE of the mth principal component** is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\sum_{i=1}^n \\left( \\sum_{j=1}^p \\phi_{jm}x_{ij} \\right)^2}{\\sum_{j=1}^p\\sum_{i=1}^nx_{ij}^2}\n",
    "\\end{align}\n",
    "\n",
    "The PVE of each principal component is a positive quantity. In order to\n",
    "compute the **cumulative PVE** of the first $M$ principal components, we\n",
    "can simply sum (10.8) over each of the first $M$ PVEs. In total, there are\n",
    "$min(n − 1, p)$ principal components, and their PVEs sum to one.\n",
    "\n",
    "<img src=\"./images/3.png\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding How Many Principal Components to Use\n",
    "\n",
    "We would like to use the smallest number of principal components required to get a good understanding of the\n",
    "data. \n",
    "\n",
    "**How many principal components are needed?**\n",
    "- We typically decide on the number of principal components required to visualize the data by examining a **scree plot** (Right FIGURE 10.4)\n",
    "- We choose the smallest number of\n",
    "principal components that are required in order to explain a sizable amount\n",
    "of the variation in the data.\n",
    "- We tend to look\n",
    "at the first few principal components in order to find interesting patterns\n",
    "in the data. If no interesting patterns are found in the first few principal\n",
    "components, then further principal components are unlikely to be of interest.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
