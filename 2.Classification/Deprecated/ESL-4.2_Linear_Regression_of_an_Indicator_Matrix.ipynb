{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Linear Regression of an Indicator Matrix\n",
    "\n",
    "#### Indicator response matrix $\\mathbf{Y}$:\n",
    "- N $\\times$ K matrix with N training instances of form Y = (Y1,...,YK); Yk = 1 if G = k else 0\n",
    "- Y is a matrix of 0's and 1's, with each row having a single 1\n",
    "\n",
    "We fit a linear regression model to each of the columns of Y simultaneously, and the fit is given by:\n",
    "\\begin{align}\n",
    "\\mathbf{\\hat{Y}}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty\n",
    "\\end{align}\n",
    "\n",
    "Note that we have a coefficient vector for each response column $y_k$, and hence a (p+1)$\\times$K coefficient matrix $\\mathbf{\\hat{B}}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^Ty$. Here X is the model matrix with p+1 columns\n",
    "corresponding to the p inputs, and a leading column of 1's for the intercept.\n",
    "\n",
    "A new observation with input x is classified as follows:\n",
    "- compute the fitted output $\\hat{f}(x)=[(1,x)\\mathbf{\\hat{B}}]^T$, a K vector\n",
    "- identify the largest component and classify accordingly:\n",
    "\\begin{align}\n",
    "\\hat{G}(x)=argmax_{k}\\hat{f}_k(x)\n",
    "\\end{align}\n",
    "\n",
    "### Rationale\n",
    "One rather formal justificationis to view the regression as *an estimate of conditional expectation.*\n",
    "\n",
    "### A more simplistic viewpoint\n",
    "Construct targets $t_k$ for each class, where $t_k$ is the kth column of the K \u0002 K identity matrix. Our prediction problem is to try and reproduce the appropriate target for an observation. With the same coding as before, the response vector yi (ith row of Y) for observation i has the value $y_i=t_k$ if $g_i=k$. We might then fit the linear model by least squares:\n",
    "\n",
    "\\begin{align}\n",
    "\\min_B \\sum_{i=1}^N||y_i-[(1,x_i)B]^T||^2\n",
    "\\end{align}\n",
    "\n",
    "The criterion is a sum-of-squared Euclidean distances of the fitted vector from their targets. A new observation is classified by computing its fitted vector $\\hat{f}(x)$ and classifying to the closest target:\n",
    "\\begin{align}\n",
    "\\hat{G}(x)=argmin_k||\\hat{f}(x)-t_k||^2\n",
    "\\end{align}\n",
    "\n",
    "#### The number of classes K >=3\n",
    "A loose but general rule is that if K >=3 classes are lined up, polynomial terms up to degree K -1 might be needed to resolve them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
