{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Logistic Regression, LDA, QDA, and KNN \n",
    "\n",
    "## logistic regression and LDA methods are closely connected.\n",
    "\n",
    "**Setting**:\n",
    " Consider the two-class setting with $p = 1$ predictor,\n",
    "and let $p_1(x)$ and $p_2(x) = 1−p_1(x)$ be the probabilities that the observation\n",
    "$X = x$ belongs to class 1 and class 2, respectively.\n",
    "\n",
    "In LDA, from\n",
    "\n",
    "\\begin{align}\n",
    "p_k(x)=\\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left( -\\frac{1}{2\\sigma^2}(x-\\mu_k)^2 \\right)}}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{\\left( -\\frac{1}{2\\sigma^2}(x-\\mu_l)^2 \\right)}}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\delta_k(x)=x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\log(\\pi_k) \n",
    "\\end{align}\n",
    "\n",
    "The **log odds** is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\log{\\frac{p_1(x)}{1-p_1(x)}}=\\log{\\frac{p_1(x)}{p_2(x)}}=c_0+c_1x\n",
    "\\end{align}\n",
    "\n",
    "where c0 and c1 are functions of μ1, μ2, and σ2.\n",
    "\n",
    "In Logistic Regression,\n",
    "\n",
    "\\begin{align}\n",
    "\\log{\\frac{p_1}{1-p_1}}=\\beta_0+\\beta_1x\n",
    "\\end{align}\n",
    "\n",
    "**SAME**\n",
    "\n",
    " - **Both logistic regression\n",
    "and LDA produce linear decision boundaries.** \n",
    "\n",
    "**DIFFERENCES**\n",
    "\n",
    " - The only difference\n",
    "between the two approaches lies in the fact that β0 and β1 are estimated\n",
    "using maximum likelihood, whereas c0 and c1 are computed using the estimated\n",
    "mean and variance from a normal distribution. This same connection\n",
    "between LDA and logistic regression also holds for multidimensional data\n",
    "with p > 1.\n",
    "\n",
    " - LDA assumes that the observations are drawn\n",
    "from a Gaussian distribution with a common covariance matrix in each\n",
    "class, and so can provide some improvements over logistic regression when\n",
    "this assumption approximately holds. Conversely, logistic regression can\n",
    "outperform LDA if these Gaussian assumptions are not met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN dominate LDA and Logistic in non-linear setting\n",
    "\n",
    "In order to make a prediction for an observation X = x, the K training observations that are closest to x are\n",
    "identified. Then X is assigned to the class to which the plurality of these\n",
    "observations belong. Hence KNN is a completely **non-parametric** approach:\n",
    "*no assumptions are made about the shape of the decision boundary*. \n",
    "\n",
    ">Therefore,\n",
    "we can expect KNN to dominate LDA and logistic regression\n",
    "when the decision boundary is highly non-linear. \n",
    "\n",
    "On the other hand, KNN\n",
    "does not tell us which predictors are important\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDA serves as a compromise between KNN, LDA and logistic regression\n",
    "\n",
    "QDA serves as a compromise between the non-parametric KNN\n",
    "method and the linear LDA and logistic regression approaches. Since QDA\n",
    "assumes a quadratic decision boundary, it can accurately model a wider\n",
    "range of problems than can the linear methods. Though not as flexible\n",
    "as KNN, QDA can perform better in the presence of a *limited number of\n",
    "training observations* because it does make some assumptions about the\n",
    "form of the decision boundary. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/17.png\" width=800> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario 1**: \n",
    " - 20 training observations in each of two classes. The observations within each class were uncorrelated random normal variables with a different mean in each class.\n",
    " - LDA performed well in this setting. KNN performed poorly because it paid a price in terms of variance that\n",
    "was not offset by a reduction in bias. \n",
    "\n",
    "**Scenario 2**: \n",
    " - Details are as in Scenario 1, except that within each\n",
    "class, the two predictors had a correlation of −0.5. \n",
    " - Little change in the relative performances of\n",
    "the methods as compared to the previous scenario.\n",
    "\n",
    "**Scenario 3**: \n",
    " - X1 and X2 are from the t-distribution, with 50 observations per class. \n",
    " \n",
    "> The **t-distribution** has a similar shape to \n",
    "the normal distribution, but it has a tendency to yield more extreme\n",
    "points—that is, more points that are far from the mean. \n",
    "\n",
    " - The decision boundary was still linear, and so fit into the logistic\n",
    "regression framework. The set-up violated the assumptions of LDA,\n",
    "since the observations were not drawn from a normal distribution. QDA results deteriorated considerably\n",
    "as a consequence of non-normality.\n",
    "\n",
    "**Scenario 4**: \n",
    " - The data were generated from a normal distribution,\n",
    "with a correlation of 0.5 between the predictors in the first class,\n",
    "and correlation of −0.5 between the predictors in the second class.\n",
    " - This setup corresponded to the QDA assumption, and resulted in\n",
    "quadratic decision boundaries. \n",
    "\n",
    "**Scenario 5**: \n",
    " - Within each class, the observations were generated from\n",
    "a normal distribution with uncorrelated predictors. However, the responses\n",
    "were sampled from the logistic function using $X^2_1 , X^2_2, and \\, X1 × X2$ as predictors. \n",
    " - Consequently, there is a quadratic decision\n",
    "boundary. QDA once\n",
    "again performed best, followed closely by KNN-CV. The linear methods\n",
    "had poor performance.\n",
    "\n",
    "**Scenario 6**: \n",
    " - Details are as in the previous scenario, but the responses\n",
    "were sampled from a more complicated non-linear function. \n",
    " - Even the quadratic decision boundaries of QDA could not adequately\n",
    "model the data. \n",
    " - Much more flexible KNN-CV method gave the best results. But\n",
    "KNN with K = 1 gave the worst results out of all methods. \n",
    "\n",
    ">This\n",
    "highlights the fact that **even when the data exhibits a complex nonlinear\n",
    "relationship, a non-parametric method such as KNN can still\n",
    "give poor results if the level of smoothness is not chosen correctly.**\n",
    "\n",
    "## Conclusion\n",
    "- When the true decision boundaries are linear, then\n",
    "the LDA and logistic regression approaches will tend to perform well.\n",
    "\n",
    "- When\n",
    "the boundaries are moderately non-linear, QDA may give better results.\n",
    "\n",
    "- For much more complicated decision boundaries, a non-parametric\n",
    "approach such as KNN can be superior. But the level of smoothness for a\n",
    "non-parametric approach must be chosen carefully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
