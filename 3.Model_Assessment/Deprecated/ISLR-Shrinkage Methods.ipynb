{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shrinkage Methods v.s. Subset Selection**:\n",
    "- Subset selection methods described involve using least\n",
    "squares to fit a linear model that contains a subset of the predictors.\n",
    "- Shrinkage Methods fit a model containing all p predictors by constraining or regularizing the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "Recall least squares:\n",
    "\n",
    "\\begin{align}\n",
    "RSS=\\sum_{i=1}^n\\left( y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j \\right)^2\n",
    "\\end{align}\n",
    "\n",
    "**Ridge regression** coefficient estimates $\\hat{\\beta}^R$ are the values that minimize\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^n\\left( y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j \\right)^2+\\lambda\\sum_{j=1}^p\\beta_j^2=RSS+\\lambda\\sum_{j=1}^p\\beta_j^2\n",
    "\\end{align}\n",
    "\n",
    "**Trade-off:**\n",
    "1. Ridge regression seeks coefficient estimates that fit the data well, by making the RSS\n",
    "small. \n",
    "2. **shrinkage penalty** $\\lambda\\sum_{j=1}^p\\beta_j^2$ is small when β1, . . . , βp are close to zero, and so it has the effect of shrinking the estimates of βj towards zero\n",
    "\n",
    "**Standardization**:\n",
    "\n",
    "- **scale equivariant**: The standard least squares coefficient estimates are scale equivariant: multiplying Xj by a constant c simply leads to a\n",
    "scale\n",
    "scaling of the least squares coefficient estimates by a factor of 1/c.\n",
    "\n",
    "- $X_{j,\\lambda}^\\beta$ will depend not only on the value of λ, but also on the scaling of the jth predictor, and the scaling of the other predictors. It is best to apply ridge regression after\n",
    "standardizing the predictors\n",
    "\\begin{align}\n",
    "\\tilde{x}_{ij}=\\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}}\n",
    "\\end{align}\n",
    "The denominator is the\n",
    "estimated standard deviation of the jth predictor\n",
    "\n",
    "## Ridge Regression Improves Over Least Squares\n",
    "1. **bias-variance\n",
    "trade-off**\n",
    " - Ridge regression’s advantage over least squares is rooted in the bias-variance\n",
    "trade-off. As λ increases, the flexibility of the ridge regression fit decreases,\n",
    "leading to decreased variance but increased bias.\n",
    " - At the least squares coefficient estimates, which correspond\n",
    "to ridge regression with λ = 0, the variance is high but there is no bias. But\n",
    "as λ increases, the shrinkage of the ridge coefficient estimates leads to a\n",
    "substantial reduction in the variance of the predictions, at the expense of a\n",
    "slight increase in bias.\n",
    "<img src=\"./images/4.png\" width=600>\n",
    "\n",
    " > ridge regression works best in situations\n",
    "where the least squares estimates have high variance\n",
    "\n",
    "2. **computational advantages over best subset selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Lasso\n",
    "\n",
    "The lasso coefficients, $\\hat{\\beta}_\\lambda^L$, minimize the quantity\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^n\\left( y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j \\right)^2+\\lambda\\sum_{j=1}^p|\\beta_j|=RSS+\\lambda\\sum_{j=1}^p|\\beta_j|\n",
    "\\end{align}\n",
    "\n",
    "## Another Formulation for Ridge Regression and the Lasso\n",
    "The lasso and ridge regression coefficient estimates solve\n",
    "the problems\n",
    "\n",
    "\\begin{align}\n",
    "minimize_\\beta \\left\\{\\sum_{i=1}^n\\left( y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j \\right)^2\\right\\}\\,\\, subject\\, to \\, \\sum_{j=1}^p|\\beta_j|\\leq s \\\\\n",
    "minimize_beta \\left\\{\\sum_{i=1}^n\\left( y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j \\right)^2\\right\\}\\,\\, subject\\, to \\, \\sum_{j=1}^p\\beta_j^2\\leq s\n",
    "\\end{align}   \n",
    "\n",
    "When we perform the lasso we are trying\n",
    "to find the set of coefficient estimates that lead to the smallest RSS, subject\n",
    "to the constraint that there is a budget s for how large $\\sum_{j=1}^p|\\beta_j|$ can be.\n",
    "When s is extremely large, then this budget is not very restrictive, and so\n",
    "the coefficient estimates can be large\n",
    "\n",
    "**A close connection between the lasso, ridge regression, and best subset selection**:\n",
    "\n",
    "best subset selection is equivelant to :\n",
    "\\begin{align}\n",
    "minimize_beta \\left\\{\\sum_{i=1}^n\\left( y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j \\right)^2\\right\\}\\,\\, subject\\, to \\, \\sum_{j=1}^pI(\\beta_j\\neq 0)\\leq s\n",
    "\\end{align}  \n",
    "\n",
    "Therefore, we can interpret **ridge regression** and **the\n",
    "lasso** as computationally feasible alternatives to **best subset selection**.\n",
    "\n",
    "## The Variable Selection Property of the Lasso\n",
    "The lasso and ridge regression coefficient estimates are given by the\n",
    "first point at which an ellipse contacts the constraint region. \n",
    "\n",
    "**ridge regression**: **circular** constraint with no sharp points, so the ridge regression coefficient\n",
    "estimates will be exclusively non-zero. \n",
    "\n",
    "**the lasso**: constraint has **corners** at each of the axes, and so the ellipse will often intersect the constraint\n",
    "region at an axis. \n",
    " - the $l_1$ penalty has the effect\n",
    "of forcing some of the coefficient estimates to be exactly equal to zero when\n",
    "the tuning parameter λ is sufficiently large. \n",
    " - Hence, much like best subset selection,\n",
    "the lasso performs **variable selection**\n",
    "\n",
    "> lasso yields **sparse** models\n",
    "\n",
    "<img src=\"./images/5.png\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Lasso and Ridge Regression\n",
    "\n",
    "**SAME**: Ridge & Lasso all can yield a reduction in variance\n",
    "at the expense of a small increase in bias, and consequently can generate\n",
    "more accurate predictions. \n",
    "\n",
    "**DIFFERENCES**: \n",
    " - Unlike ridge regression, the **lasso performs\n",
    "variable selection**, and hence results in models that are easier to interpret.\n",
    " - ridge\n",
    "regression outperforms the lasso in terms of prediction error in this setting\n",
    "\n",
    "\n",
    "**Suitable setting**:\n",
    "- **Lasso**: perform better in a setting where a relatively small number of predictors\n",
    "have substantial coefficients, and the remaining predictors have coefficients\n",
    "that are very small or that equal zero.\n",
    "- **Ridge regression**: perform better\n",
    "when the response is a function of many predictors, all with coefficients of\n",
    "roughly equal size.\n",
    "- The number of predictors that is related to the\n",
    "response is never known a **priori** for real data sets. Cross-validation can be used in order to determine which approach is better\n",
    "on a particular data set.\n",
    "\n",
    "<img src=\"./images/6.png\" width=600>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
