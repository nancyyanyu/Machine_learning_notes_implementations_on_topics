{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Dimension Reduction Methods\n",
    "\n",
    "Subset selection and shrinkage methods all use the original predictors, X1,X2, . . . , Xp. \n",
    "\n",
    "Dimension Reduction Methods ***transform*** the predictors and then fit a least\n",
    "squares model using the transformed variables.\n",
    "\n",
    "## Approach\n",
    "Let Z1,Z2, . . . ,ZM represent M < p linear combinations of our original p predictors. That is,\n",
    "\n",
    "\\begin{align}\n",
    "Z_m=\\sum_{j=1}^p\\phi_{jm}X_j\n",
    "\\end{align}\n",
    "\n",
    "for some constants φ1m, φ2m . . . , φpm, m = 1, . . .,M. We can then fit the\n",
    "linear regression model\n",
    "\n",
    "\\begin{align}\n",
    "y_i=\\theta_0+\\sum_{m=1}^M\\theta_m z_{im}+\\epsilon_i  \\quad  i=1,2,3,4,...,n\n",
    "\\end{align}\n",
    "\n",
    "**Dimension reduction**: reduces the problem of estimating the p+1 coefficients β0, β1, . . . , βp to the\n",
    "simpler problem of estimating the M + 1 coefficients θ0, θ1, . . . , θM, where\n",
    "M < p. In other words, the dimension of the problem has been reduced\n",
    "from p + 1 to M + 1.\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{m=1}^M\\theta_m z_{im}&=\\sum_{m=1}^M\\theta_m \\sum_{j=1}^p\\phi_{jm}x_{ij}=\\sum_{m=1}^M\\sum_{j=1}^p\\theta_m \\phi_{jm}x_{ij}=\\sum_{j=1}^p \\beta_jx_{ij}  \\\\\n",
    "\\beta_j&=\\sum_{m=1}^M\\theta_m \\phi_{jm}\n",
    "\\end{align}\n",
    "\n",
    "**All dimension reduction methods work in two steps:**\n",
    "\n",
    "1. the transformed\n",
    "predictors Z1, Z2, . . . , ZM are obtained.\n",
    "2. the model is fit\n",
    "using these M predictors. However, the choice of Z1, Z2, . . . , ZM, or equivalently,\n",
    "the selection of the φjm’s, can be achieved in different ways. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Regression \n",
    "\n",
    "\n",
    "## An Overview of Principal Components Analysis\n",
    "\n",
    "**PCA**: is a technique for reducing the dimension of a n × p data matrix X.\n",
    "\n",
    "### 1st Principal Component\n",
    "\n",
    "#### Interpretation 1: greatest variability\n",
    "\n",
    "**The first principal component** direction of the data: is that along which the\n",
    "observations **vary the most**.\n",
    "\n",
    "<img src=\"./images/7.png\" width=600>\n",
    "\n",
    "The first principal component direction is the direction along which there is the greatest\n",
    "variability in the data. That is, if we projected the 100 observations onto\n",
    "this line (as shown in the left-hand panel of Figure 6.15), then the resulting\n",
    "projected observations would have the largest possible variance\n",
    "\n",
    "<img src=\"./images/8.png\" width=600>\n",
    "\n",
    "The first principal component is given by the formula\n",
    "\n",
    "\\begin{align}\n",
    "Z_1 = 0.839 × (pop − \\bar{pop}) + 0.544 × (ad − \\bar{ad})\n",
    "\\end{align}\n",
    "\n",
    "Here φ11 = 0.839 and φ21 = 0.544 are the **principal component loadings**,\n",
    "which define the direction referred to above.\n",
    "\n",
    ">The idea is that out of every possible linear combination\n",
    "of pop and ad such that $\\phi_{11}^2+\\phi_{21}^2=1$, this particular linear combination\n",
    "yields the highest variance: i.e. this is the linear combination for which\n",
    "$Var(φ_{11} × (pop − \\bar{pop}) + φ_{21} × (ad − \\bar{ad}))$ is maximized.\n",
    "\n",
    "**Principal Component Scores**\n",
    "\n",
    "The values of $z_{11}, . . . , z_{n1}$ are known as the **principal component scores**, and\n",
    "can be seen in the right-hand panel of Figure 6.15. For example,\n",
    "\n",
    "\\begin{align}\n",
    "z_{i1} = 0.839 × (pop_i − \\bar{pop}) + 0.544 × (ad_i − \\bar{ad})\n",
    "\\end{align}\n",
    "\n",
    "#### Interpretation 2: closest to data\n",
    "\n",
    "\n",
    "There is also another interpretation for PCA: the first principal component\n",
    "vector defines the line that is as close as possible to the data. \n",
    "\n",
    "In Figure 6.14, the first principal component line minimizes the\n",
    "sum of the squared perpendicular distances between each point and the\n",
    "line.\n",
    "\n",
    "In the right-hand panel of Figure 6.15, the left-hand panel has been\n",
    "rotated so that the first principal component direction coincides with the\n",
    "x-axis. It is possible to show that the ***first principal component score*** for\n",
    "the ith observation is the distance in the x-direction of the\n",
    "ith cross from zero.\n",
    "\n",
    "#### Interpretation 3: single number summarization\n",
    "\n",
    "We can think of the values of the principal component Z1 as single number\n",
    "summaries of the joint pop and ad budgets for each location. \n",
    "\n",
    "In\n",
    "this example, if $z_{i1} = 0.839 × (pop_i − pop) + 0.544 × (ad_i − ad) < 0$,\n",
    "then this indicates a city with below-average population size and belowaverage\n",
    "ad spending.\n",
    "\n",
    "<img src=\"./images/9.png\" width=650>\n",
    "\n",
    "Figure 6.16 displays\n",
    "$z_{i1}$ versus both pop and ad. The plots show a strong relationship between\n",
    "the first principal component and the two features. In other words, the first\n",
    "principal component appears to *capture most of the information* contained\n",
    "in the pop and ad predictors.\n",
    "\n",
    "### 2nd Principal Component\n",
    "\n",
    "The second\n",
    "principal component Z2 is a linear combination of the variables that is uncorrelated\n",
    "with Z1, and has largest variance subject to this constraint.\n",
    "\n",
    "It turns out that the zero correlation condition of Z1 with Z2 is equivalent to the condition that the direction must be perpendicular, or orthogonal, to the first principal component direction. \n",
    "\n",
    "The second principal\n",
    "component is given by the formula:\n",
    "\n",
    "\\begin{align}\n",
    "Z_2 = 0.544 × (pop − \\bar{pop}) − 0.839 × (ad − \\bar{ad}).\n",
    "\\end{align}\n",
    "\n",
    "Figure 6.15. The fact that the\n",
    "second principal component scores are much closer to zero indicates that\n",
    "this component captures far less information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Principal Components Regression Approach\n",
    "\n",
    "The principal components regression (PCR) approach involves constructing\n",
    "principal\n",
    "components\n",
    "regression\n",
    "the first M principal components, Z1, . . ., ZM, and then using these components\n",
    "as the predictors in a linear regression model that is fit\n",
    "using least squares\n",
    "\n",
    "\n",
    "**The key idea**\n",
    "\n",
    "Often a small number of principal\n",
    "components suffice to explain most of the variability in the data, as\n",
    "well as the relationship with the response. In other words, we assume that\n",
    "***the directions in which X1, . . .,Xp show the most variation are the directions\n",
    "that are associated with Y***\n",
    "\n",
    "**Example**\n",
    "\n",
    "<img src=\"./images/10.png\" width=650>\n",
    "\n",
    "- Performing PCR with an appropriate\n",
    "choice of M can result in a substantial improvement over least squares\n",
    "- PCR does not\n",
    "perform as well as the two shrinkage methods\n",
    " - **Reason**: The data were generated in such a way that many principal\n",
    "components are required in order to adequately model the response.\n",
    "In contrast, PCR will tend to do well in cases when the first few principal\n",
    "components are sufficient to capture most of the variation in the predictors\n",
    "as well as the relationship with the response.\n",
    "\n",
    "**Note**: even though PCR provides a simple way to perform\n",
    "regression using M < p predictors, it is not a *feature selection* method!\n",
    "\n",
    " - This is because each of the M principal components used in the regression is a linear combination of all p of the original features.\n",
    " - PCR is more closely related to ridge regression than\n",
    "to the lasso. One can even think of ridge regression as a continuous version\n",
    "of PCR!\n",
    "\n",
    "**Cross-validation**: In PCR, the number of principal components, M, is typically chosen by\n",
    "cross-validation.\n",
    "\n",
    "<img src=\"./images/11.png\" width=650>\n",
    "\n",
    "**Standardisation**: When performing PCR, we generally recommend standardizing each\n",
    "predictor, prior to generating the principal components.\n",
    " - In the\n",
    "absence of standardization, the *high-variance variables* will tend to play a\n",
    "larger role in the principal components obtained, and the scale on which\n",
    "the variables are measured will ultimately have an effect on the final PCR\n",
    "model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
