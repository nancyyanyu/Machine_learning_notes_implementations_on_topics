{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Dimensional Data\n",
    "\n",
    "**High-dimensional**: Data sets containing more features than observations are often referred\n",
    "to as high-dimensional.\n",
    "- Classical approaches such as least squares linear\n",
    "highregression\n",
    "are not appropriate in this setting\n",
    "\n",
    "# What Goes Wrong in High Dimensions?\n",
    "\n",
    "1. When the number of features p is as large as, or >n, least squares cannot be performed.\n",
    " \n",
    " **Reason**: regardless of whether or\n",
    "not there truly is a relationship between the features and the response,\n",
    "least squares will yield a set of coefficient estimates that result in a perfect\n",
    "fit to the data, such that the residuals are zero.\n",
    " - This perfect fit will almost certainly lead to\n",
    "overfitting of the data\n",
    " - The problem is simple: when p > n or p ≈ n, a simple least\n",
    "squares regression line is too ***flexible*** and hence overfits the data.\n",
    "\n",
    "\n",
    "\n",
    "2. Examines only the R2 or the training set MSE might erroneously conclude that the model with the greatest number\n",
    "of variables is best.\n",
    "<img src=\"./images/12.png\" width=650>\n",
    "\n",
    " - **Cp, AIC, and BIC** approaches are not appropriate in the high-dimensional setting, because estimating ˆσ2\n",
    "is problematic.(For instance, the formula for ˆσ2 from Chapter 3 yields an\n",
    "estimate ˆσ2 = 0 in this setting.) \n",
    " \n",
    " - **Adjusted R2** in the high-dimensional setting is problematic, since one can easily obtain\n",
    "a model with an adjusted R2 value of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression in High Dimensions\n",
    "\n",
    "**Alternative approaches better-suited to the high-dimensional setting:**\n",
    "\n",
    "- forward stepwise selection\n",
    "- ridge\n",
    "regression\n",
    "- the lasso\n",
    "- principal components regression\n",
    "\n",
    "**Reason:**\n",
    "these approaches avoid overfitting by using a less flexible fitting approach\n",
    "than least squares.\n",
    "\n",
    "**Three important points:**\n",
    "(1) regularization\n",
    "or shrinkage plays a key role in high-dimensional problems, \n",
    "\n",
    "(2) appropriate\n",
    "tuning parameter selection is crucial for good predictive performance, and\n",
    "\n",
    "(3) the test error tends to increase as the dimensionality of the problem\n",
    "(i.e. the number of features or predictors) increases, unless the additional\n",
    "features are truly associated with the response.$\\Rightarrow$ **curse of dimensionality**\n",
    "\n",
    "## Curse of dimensionality\n",
    "Adding additional signal features\n",
    "that are truly associated with the response will improve the fitted model; However, adding\n",
    "noise features that are not truly associated with the response will lead\n",
    "to a deterioration in the fitted model.\n",
    "\n",
    "**Reason**: This is because noise features increase the dimensionality of the problem, exacerbating the risk of overfitting (since noise features may be\n",
    "assigned nonzero coefficients due to chance associations with the response\n",
    "on the training set) without any potential upside in terms of improved test\n",
    "set error.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Results in High Dimensions\n",
    "1. Be cautious in reporting the results obtained when we perform the lasso, ridge regression, or other regression procedures\n",
    "in the high-dimensional setting.\n",
    " - In the high-dimensional setting, the **multicollinearity**\n",
    "problem is extreme: any variable in the model can be written as a linear\n",
    "combination of all of the other variables in the model. This\n",
    "means that we can never know exactly which variables (if any) truly are\n",
    "predictive of the outcome, and we can never identify the best coefficients\n",
    "for use in the regression.\n",
    "\n",
    "2. Be cautious in reporting errors and measures of model fit in the high-dimensional setting\n",
    " - e.g.: when p > n, it is easy to obtain a useless model that has zero residuals.\n",
    " - **One should never use sum of squared errors, p-values, R2 statistics, or other traditional measures of model fit on the training data as\n",
    "evidence of a good model fit in the high-dimensional setting**\n",
    " - It is important to\n",
    "instead report results on an independent test set, or cross-validation errors.\n",
    "For instance, the MSE or R2 on an independent test set is a valid measure\n",
    "of model fit, but the MSE on the training set certainly is not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
