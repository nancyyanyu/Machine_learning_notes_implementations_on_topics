{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Shrinkage Methods\n",
    "\n",
    "> *Subset selection methods* are discrete process—variables are either retained or discarded—it often exhibits high variance,and so doesn’t reduce the prediction error of the full model. *Shrinkage methods* are more continuous, and don’t suﬀer as much from high variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1 Ridge Regression\n",
    "\n",
    "**Ridge regression** shrinks the regression coeﬃcients by imposing a penalty on their size.The ridge coeﬃcients minimize a penalized residual sum of squares:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta}^{ridge}=argmin_\\beta {\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j)^2+\\lambda\\sum_{j=1}^p\\beta_j^2}\n",
    "\\end{align}\n",
    "\n",
    "- λ ≥ 0 is a complexity parameter that controls the amount of shrinkage\n",
    "\n",
    "Writing the criterion in matrix form:\n",
    "\n",
    "\\begin{align}\n",
    "RSS(\\lambda)=(\\mathbf{y}-\\mathbf{X}\\beta)^T(\\mathbf{y}-\\mathbf{X}\\beta)+\\lambda\\beta^T\\beta\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The ridge regression solutions:\n",
    "\\begin{align}\n",
    "\\hat{\\beta}^{ridge}=(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "- $\\mathbf{I}$ is the p×p identity matrix\n",
    "\n",
    "Note:\n",
    "- the ridge regression solution is again a linear function of $\\mathbf{y}$;\n",
    "- The solution adds a positive constant to the diagonal of $\\mathbf{X}^T\\mathbf{X}$ before inversion, which makes the problem nonsingular.\n",
    "\n",
    "### Singular value decomposition (SVD)\n",
    "The **singular value decomposition (SVD)** of the centered input matrix X gives us some additional insight into the nature of ridge regression. The SVD of the N × p matrix X has the form:\n",
    "\n",
    "\\begin{align}\n",
    "X=UDV^T\n",
    "\\end{align}\n",
    "\n",
    "- U: N×p orthogonal matrices, with the columns of U spanning the column space of X\n",
    "- V: p×p orthogonal matrices, the columns of V spanning the row space of X\n",
    "- D: p×p diagonal matrix, with diagonal entries d1 ≥ d2 ≥···≥ dp ≥ 0 called the singular values of X. If one or more values dj =0,X is singular\n",
    "\n",
    "least squares ﬁtted vector: \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{X}\\hat{\\beta}^{ls}&=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n",
    "&=UDV^T (VD^TU^TUDV^T)^{-1}VD^TU^Ty \\\\\n",
    "&=UDV^T (VD^TDV^T)^{-1}VD^TU^Ty \\\\\n",
    "&=UDV^T (V^T)^{-1}D^{-1}(D^T)^{-1}V^{-1}VD^TU^Ty \\\\\n",
    "&=\\mathbf{U}\\mathbf{U}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "Note: $\\mathbf{U}^T\\mathbf{y}$ are the coordinates of y with respect to the orthonormal basis U. \n",
    "\n",
    "The ridge solutions:\n",
    "\\begin{align}\n",
    "\\mathbf{X}\\hat{\\beta}^{ridge}&=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n",
    "&=UD(D^2+\\lambda\\mathbf{I})^{-1}D^TU^Ty \\\\\n",
    "&=\\sum_{j=1}^p\\mathbf{u}_j\\frac{d^2_j}{d^2_j+\\lambda}\\mathbf{u}^T_j\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "- $\\mathbf{u}_j$ are the columns of U\n",
    "\n",
    "Note: ridge regression computes the coordinates of y with respect to the orthonormal basis U. It then shrinks these coordinates by the factors $\\frac{d^2_j}{d^2_j+\\lambda}$\n",
    "\n",
    "#### What does a small value of $d^2_j$ mean? \n",
    "The SVD of the centered matrix X is another way of expressing the **principal components** of the variables in X. The sample covariance matrix is given by $S=X^TX/N$, we have\n",
    "\n",
    "**Eigen decomposition of $X^TX$:**\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{X}^T\\mathbf{X}=VD^TU^TUDV^T=VD^2V^T\n",
    "\\end{align}\n",
    "\n",
    "The eigenvectors $v_j$ (columns of V) are also called the **principal components** (or Karhunen–Loeve) directions of X.\n",
    "The ﬁrst principal component direction $v_1$ has the property that $z_1=Xv_1$ has the largest sample variance amongst all normalized linear combinations of the columns of X, which is:\n",
    "\n",
    "\\begin{align}\n",
    "Var(z_1)=Var(Xv_1)=\\frac{d^2_1}{N}\n",
    "\\end{align}\n",
    "\n",
    "and in fact $z_1=Xv_1=u_1d_1$. The derived variable $z_1$ is called the ﬁrst principal component of X, and hence $u_1$ is the normalized ﬁrst principal component.Subsequent principal components $z_j$ have maximum variance $\\frac{d^2_j}{N}$, subject to being orthogonal to the earlier ones.\n",
    "\n",
    "Hence the small singular values $d_j$ correspond to directions in the column space of X having small variance, and ridge regression shrinks these directions the most.\n",
    "\n",
    "### Eﬀective degrees of freedom\n",
    "\\begin{align}\n",
    "df(\\lambda)&=tr[\\mathbf{X}(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T] \\\\\n",
    "&=tr[\\mathbf{H}\\lambda] \\\\\n",
    "&=\\sum^p_{j=1}\\frac{d^2_j}{d^2_j+\\lambda}\n",
    "\\end{align}\n",
    "\n",
    "This monotone decreasing function of λ is the eﬀective degrees of freedom of the ridge regression ﬁt.\n",
    "Usually in a linear-regression ﬁt with p variables,the degrees-of-freedom of the ﬁt is p, the number of free parameters.\n",
    "\n",
    "Note that\n",
    "> df(λ)= p as λ = 0 (no regularization)\n",
    "\n",
    "> df(λ) → 0 as λ →∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 The Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lasso is a shrinkage method like ridge, with subtle but important differences.The lasso estimate is deﬁned by:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta}^{lasso}&=argmin_\\beta\\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j)^2 \\\\\n",
    "& s.t. \\sum_{j=1}^p|\\beta_j|\\leq t\n",
    "\\end{align}\n",
    "\n",
    "Lasso problem in *Lagrangian form*:\n",
    "\\begin{align}\n",
    "\\hat{\\beta}^{lasso}&=argmin_\\beta\\{ \\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j)^2+\\lambda\\sum_{j=1}^p|\\beta_j| \\}\n",
    "\\end{align}\n",
    "\n",
    "#### Difference with ridge:\n",
    "The L2 ridge penalty $\\sum_{j=1}^p\\beta_j^2$ is replaced by the L1 lasso penalty $\\sum_{j=1}^p|\\beta_j|$. This\n",
    "latter constraint makes the solutions nonlinear in the $y_i$, and there is no closed form expression as in ridge regression.\n",
    "\n",
    "> t should be adaptively chosen to minimize an estimate of expected prediction error.\n",
    "\n",
    "- if $t>t_0=\\sum_{j=1}^p|\\hat{\\beta_j^{ls}}|$, then the lasso estimates are the $\\hat{\\beta_j^{ls}}$\n",
    "- if $t>t_0/2$, the least squares coeﬃcients are shrunk by about 50% on average\n",
    "\n",
    "The standardized parameter: $s=t/\\sum_1^p|\\hat{\\beta_j}|$\n",
    "\n",
    "- s=1.0,  the lasso coeﬃcients  are the least squares estimates\n",
    "- s->0, as the lasso coeﬃcients ->0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.3 Discussion: Subset Selection, Ridge Regression and the Lasso\n",
    "\n",
    "- Ridge regression: does a proportional shrinkage\n",
    "- Lasso: translates each coeﬃcient by a constant factor λ, truncating at zero --“soft thresholding,”\n",
    "- Best-subset selection: drops all variables with coeﬃcients smaller than the Mth largest --“hard-thresholding.”\n",
    "<img src=\"./images/lass_ridge.png\",width=550>\n",
    "\n",
    "### Bayes View\n",
    "\n",
    "Consider the criterion\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{\\beta}&=argmin_\\beta\\{ \\sum_{i=1}^N(y_i-\\beta_0-\\sum_{j=1}^px_{ij}\\beta_j)^2+\\lambda\\sum_{j=1}^p|\\beta_j|^q \\}\n",
    "\\end{align}\n",
    "\n",
    "for q ≥ 0. The contours of constant value of $\\sum_{j=1}^p|\\beta_j|^q$ are shown in Figure 3.12, for the case of two inputs.\n",
    "<img src=\"./images/q.png\",width=550>\n",
    "\n",
    "<font color= 'red'>The lasso, ridge regression and best subset selection are Bayes estimates with diﬀerent priors:</font>Thinking of $\\sum_{j=1}^p|\\beta_j|^q$ as the log-prior density for βj , these are also the equi-contours of the prior distribution of the parameters. \n",
    "\n",
    "- q = 0 :variable subset selection, as the penalty simply counts the number of nonzero parameters;\n",
    "- q = 1 :the lasso, also Laplace distribution for each input, with density $\\frac{1}{2\\tau}exp(-|\\beta|/\\tau)$, where $\\tau=1/\\lambda$\n",
    "\n",
    "- q = 2 :the ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
