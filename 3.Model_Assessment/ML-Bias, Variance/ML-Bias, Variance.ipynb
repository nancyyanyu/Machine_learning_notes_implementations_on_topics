{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Study Note: Bias, Variance and Model Complexity\"\n",
    "date: '2019–06–08 00:00:16'\n",
    "categories: Machine Learning\n",
    "mathjax: true\n",
    "abbrlink: 2a71b2a0\n",
    "tags:\n",
    "- Model Assessment\n",
    "- Model Selection\n",
    "comments: true\n",
    "---\n",
    "\n",
    "# Bias, Variance and Model Complexity\n",
    "\n",
    "**Test error** (generalization error): the prediction error over an independent test sample\n",
    "$$\n",
    "𝐸𝑟𝑟𝜏=𝐸[𝐿(𝑌,\\hat{f} (𝑋))|𝜏]\n",
    "$$\n",
    "Here the training set $\\tau$ is fixed, and test error refers to the error for this specific training set.\n",
    "\n",
    "<!--more-->\n",
    "\n",
    "\n",
    "\n",
    "**Expected test error: **\n",
    "$$\n",
    "Err=E[L(Y,\\hat{f}(X)]=E[Err_\\tau]\n",
    "$$\n",
    "\n",
    "This expectation averages over everything that is random, including the randomness in the training set that produced $\\hat{f}$\n",
    "\n",
    "**Training error**: the average loss over the training sample\n",
    "$$\n",
    "\\bar{err}=\\frac{1}{N}\\sum_{i=1}^NL(y_i,\\hat{f}(x_i))\n",
    "$$\n",
    "\n",
    "\n",
    "<img src=\"./bv.PNG\" width=\"370\" />\n",
    "\n",
    "\n",
    "**Model selection:** estimating the performance of different models in order to choose the best one.\n",
    "\n",
    "**Model assessment:** having chosen a final model, estimating its prediction error (generalization error) on new data.\n",
    "\n",
    "Randomly divide the dataset into three parts: \n",
    "- a **training set**: fit the models\n",
    "- a **validation set**: estimate prediction error for model selection\n",
    "- a **test set**: assessment of the generalization error of the \f",
    "nal chosen model\n",
    "\n",
    "A typical split might be 50% for training, and 25% each for validation and testing:\n",
    "\n",
    "# The Bias Variance Decomposition\n",
    "\n",
    "## General Model\n",
    "If we assume that $Y=f(X)+\\epsilon$ where $E(\\epsilon)=0$, and $Var(\\epsilon)=\\sigma^2_\\epsilon$, we can derive an expression for the expected prediction error of a regression fit $\\hat{f}(X)$ at an input point X = x0, using squared-error loss:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Err(x_0)&=E[(Y-\\hat{f}(x_0))^2|X=x_0] \\\\\n",
    "&=E[(f(x_0)+\\epsilon-\\hat{f}(x_0))^2] \\\\\n",
    "&=E[\\epsilon^2+(f(x_0)-\\hat{f}(x_0))^2+2\\epsilon(f(x_0)-\\hat{f}(x_0))] \\\\\n",
    "&=\\sigma^2_\\epsilon+E[f(x_0)^2+\\hat{f}(x_0)^2-2f(x_0)\\hat{f}(x_0)] \\\\\n",
    "&=\\sigma^2_\\epsilon+E[\\hat{f}(x_0)^2]+f(x_0)^2-2f(x_0)E[\\hat{f}(x_0)]  \\\\\n",
    "&=\\sigma^2_\\epsilon+(E[\\hat{f}(x_0)])^2+f(x_0)^2-2f(x_0)E[\\hat{f}(x_0)] +E[\\hat{f}(x_0)^2]-(E[\\hat{f}(x_0))^2 \\\\\n",
    "&=\\sigma^2_\\epsilon+(E\\hat{f}(x_0)-f(x_0))^2+Var(\\hat{f}(x_0))\\\\\n",
    "&=\\sigma^2_\\epsilon+Bias^2(\\hat{f}(x_0))+Var(\\hat{f}(x_0))\\\\\n",
    "&= Irreducible Error+ Bias^2 + Variance\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "1. The first term is the variance of the target around its true mean f(x0), and cannot be avoided no matter how well we estimate f(x0), unless $\\sigma^2_\\epsilon=0$\n",
    "2. The second term is the squared bias, the amount by which the average of our estimate differs from the true mean\n",
    "3. The last term is the variance; the expected squared deviation of $\\hat{f}(x_0)$ around its mean. \n",
    "\n",
    ">Typically the more complex we make the model $\\hat{f}$, the lower the (squared) bias but the higher the variance.\n",
    "\n",
    "## KNN regression\n",
    "For the k-nearest-neighbor regression \f",
    "t, these expressions have the sim-\n",
    "ple form\n",
    "$$\n",
    "\\begin{align}\n",
    "Err(x_0)&=E[(Y-\\hat{f}_k(x_0))^2|X=x_0] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "<img src=\"./bv2.PNG\" width=\"470\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "**Ref:**\n",
    "\n",
    "James, Gareth, et al. *An introduction to statistical learning*. Vol. 112. New York: springer, 2013.\n",
    "\n",
    "Hastie, Trevor, et al. \"The elements of statistical learning: data mining, inference and prediction.\" *The Mathematical Intelligencer* 27.2 (2005): 83-85"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
