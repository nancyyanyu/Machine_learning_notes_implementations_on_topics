{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Ideas\n",
    "Tree-based methods partition the feature space into a set of rectangles, and\n",
    "then fit a simple model (like a constant) in each one.\n",
    "\n",
    "Decision tree builds regression or classification models in the form of a tree structure. \n",
    "\n",
    "- It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. \n",
    "- The final result is a tree with decision nodes and leaf nodes. \n",
    " - A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing values for the attribute tested. \n",
    " - Leaf node (e.g., Hours Played) represents a decision on the numerical target. \n",
    " - The topmost decision node in a tree which corresponds to the best predictor called root node. \n",
    "- Decision trees can handle both categorical and numerical data. \n",
    "\n",
    "<img src=\"./images/dt1.png\" width=350>\n",
    "<img src=\"./images/dt2.png\" width=350>\n",
    "\n",
    "# CART\n",
    "## Concept\n",
    "Our data: \n",
    "- $(x_i,y_i)$ for i=1,2,....,N, with $x_i=(x_{i1},x_{i2},...,x_{ip})$\n",
    "\n",
    "The algo needs to automatically decide on the splitting variables and split points,and also what topology (shape) the tree should have. Suppose first that we have a partition into M regions $R_1,R_2,...R_M$, and we model the response as a constant $c_m$ in each region:\n",
    "\n",
    "\\begin{align}\n",
    "f(x)=\\sum_{i=1}^M c_m I(x \\in R_m)\n",
    "\\end{align}\n",
    "\n",
    "If we adopt as our criterion minimization of the sum of squares $\\sum(y_i-f(x_i))^2$,it is easy to see that the best $\\hat{c_m}$ is just the average of $y_i$ in region $R_m$\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{c_m}=ave(y_i|x_i \\in R_m)\n",
    "\\end{align}\n",
    "\n",
    "## Greedy Algorithm\n",
    "Now finding the best binary partition in terms of minimum sum of squares is generally computationally infeasible. \n",
    "\n",
    "Hence we proceed with a *greedy algorithm*. \n",
    "\n",
    "1. Starting with all of the data, consider a splitting variable j and split point s\n",
    "2. Define the pair of half-planes\n",
    "\\begin{align}\n",
    "R_1(j,s)=\\{X|X_j\\leq s \\},  R_2(j,s)=\\{X|X_j > s \\}\n",
    "\\end{align}\n",
    "\n",
    "3. Seek the splitting variable j and split point s that solve:\n",
    "\\begin{align}\n",
    "min_{j,s} \\left[ min_{c_1}\\sum_{x_i \\in R_1(j,s)}(y_i-c_1)^2+min_{c_2}\\sum_{x_i \\in R_2(j,s)}(y_i-c_2)^2 \\right]\n",
    "\\end{align}\n",
    "\n",
    "4. For any choice j and s, the inner minimization is solved by:\n",
    "\\begin{align}\n",
    "\\hat{c_1}=ave(y_i|x_i \\in R_1(j,s)) \\\\\n",
    "\\hat{c_2}=ave(y_i|x_i \\in R_2(j,s))\n",
    "\\end{align}\n",
    "> For each splitting variable, the determination of the split point s can be done very quickly and hence by scanning through all of the inputs,determination of the best pair (j; s) is feasible.\n",
    "\n",
    "5. Having found the best split, we partition the data into the two resulting\n",
    "regions and repeat the splitting process on each of the two regions.\n",
    "\n",
    "## Pruning\n",
    "**How large should we grow the tree?**\n",
    "\n",
    "The preferred strategy is to grow a large tree $T_0$, stopping the splitting process only when some minimum node size (say 5) is reached. Then this large tree is pruned using **cost-complexity pruning**\n",
    "\n",
    "1. Define a subtree $T \\in T_0$ to be any tree that can be obtained by pruning $T_0$, that is, collapsing any number of its internal (non-terminal) nodes. \n",
    " - Index terminal nodes by $m$, with node m representing region $R_m$. \n",
    " - $|T|$: the number of terminal nodes in T. \n",
    " - $N_m=\\#\\{x_i \\in R_m\\}$\n",
    " - $\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i \\in R_m}y_i$\n",
    " - $Q_m(T)=\\frac{1}{N_m}\\sum_{x_i \\in R_m}(y_i-\\hat{c}_m)^2$\n",
    " \n",
    "2. Define cost complexity criterion:\n",
    "\\begin{align}\n",
    "C_\\alpha(T)=\\sum_{m=1}^{|T|}N_mQ_m(T)+\\alpha|T|\n",
    "\\end{align}\n",
    "\n",
    "### Intuition\n",
    "The idea is to find, for each $\\alpha$, the subtree $T_\\alpha \\in T_0$ to minimize $C_\\alpha(T)$.\n",
    "\n",
    "The tuning parameter $\\alpha \\geq0$ governs the tradeoff between tree size and its\n",
    "goodness of fit to the data. Large values of $\\alpha$ result in smaller trees $T_\\alpha$. As the notation suggests, with $\\alpha$ = 0 the solution is the full tree $T_0$.\n",
    "\n",
    "### Adaptively choose $\\alpha$\n",
    "\n",
    "> For each $\\alpha$ one can show that there is a unique smallest subtree $T_\\alpha$ that\n",
    "minimizes $C_\\alpha(T)$.\n",
    "\n",
    "- To find $T_\\alpha$ we use **weakest link pruning**: we successively collapse the internal node that produces the smallest per-node increase in $\\sum_{m}N_mQ_m(T)$, and continue until we produce the single-node (root) tree.\n",
    "- This gives a (finite) sequence of subtrees, and one can show this sequence must contain $T_\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nancy/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/nancy/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/nancy/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/nancy/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('./data/Position_Salaries.csv')\n",
    "X=df.iloc[:,1:-1].values\n",
    "y=df.iloc[:,-1:].values\n",
    "\n",
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stan_x=StandardScaler()\n",
    "stan_y=StandardScaler()\n",
    "X=stan_x.fit_transform(X)\n",
    "y=stan_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=0, splitter='best')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor=DecisionTreeRegressor(random_state=0)\n",
    "regressor.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f105c15ef28>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot decision tree regression for higher resolution and smoother curve\n",
    "X_grid=np.arange(min(X),max(X),0.1)\n",
    "X_grid=X_grid.reshape((X_grid.shape[0],1))\n",
    "plt.scatter(X,y,color='blue')\n",
    "plt.plot(X_grid,regressor.predict(X_grid),color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision  Tree Regression is nonlinear nor non-continuous regression model.The best way to visualize and non-continuous regression model si to visualize the regression model results in higher resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "## How does the algorithm split the data points?\n",
    "It uses reduction of standard deviation of the predictions. In other words, the standard deviation is decreased\n",
    "right after a split. Hence, building a decision tree is all about ﬁnding the attribute that returns the highest\n",
    "standard deviation reduction (i.e., the most homogeneous branches).  \n",
    "\n",
    "## What is the Information Gain and how does it work in Decision Trees?\n",
    "The Information Gain in Decision Tree Regression is exactly the Standard Deviation Reduction we are\n",
    "looking to reach. \n",
    "\n",
    "We calculate by how much the Standard Deviation decreases after each split. Because the\n",
    "more the Standard Deviation is decreased after a split, the more homogeneous the child nodes will be.\n",
    "\n",
    "## What is the Entropy and how does it work in Decision Trees?\n",
    "The Entropy measures the **disorder in a set**, here in a part resulting from a split. So the more homogeneous\n",
    "is your data in a part, the lower will be the entropy. The more you have splits, the more you have chance\n",
    "to ﬁnd parts in which your data is homogeneous, and therefore the lower will be the entropy (close to 0) in\n",
    "these parts. However you might still ﬁnd some nodes where the data is not homogeneous, and therefore the\n",
    "entropy would not be that small.\n",
    "\n",
    "## Does a Decision Tree make much sense in 1D?\n",
    "Not really, as we saw in the practical part of this section. In 1D (meaning one independent variable), the\n",
    "Decision Tree clearly tends to overﬁt the data. The Decision Tree would be much more relevant in higher\n",
    "dimension, but keep in mind that the implementation we made here in 1D would be exactly the same in\n",
    "higher dimension. Therefore you might want to keep that model in your toolkit in case you are dealing\n",
    "with a higher dimensional space. This will actually be the case in Part 3 - Classiﬁcation, where we will use\n",
    "Decision Tree for Classiﬁcation in 2D, which you will see turns out to be more relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
