{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging \n",
    "\n",
    "**Bootstrap aggregation**, or **bagging**, is a general-purpose procedure for reducing the\n",
    "variance of a statistical learning method, frequently used in the context of decision trees.\n",
    "\n",
    "**Averaging a set of observations reduces variance**: Recall that given a set of n independent observations Z1, . . . , Zn, each\n",
    "with variance $σ^2$, the variance of the mean $\\bar{Z}$ of the observations is given\n",
    "by $σ^2/n$. \n",
    "- A natural way to reduce the variance and hence increase the prediction\n",
    "accuracy of a statistical learning method is to **take many training sets\n",
    "from the population**, build a separate prediction model using each training\n",
    "set, and average the resulting predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Bootstrap** taking repeated samples from the (single) training data set \n",
    "\n",
    "**Bagging**\n",
    "- Generate B different bootstrapped training data sets. \n",
    "- Train our method on\n",
    "the bth bootstrapped training set in order to get $\\hat{f}^{*b}(x)$\n",
    "- Finally average\n",
    "all the predictions, to obtain\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{f}_{bag}(x)=\\frac{1}{B}\\sum_{b=1}^B\\hat{f}^{*b}(x)\n",
    "\\end{align}\n",
    "\n",
    "**Apply bagging to regression trees**\n",
    "- Construct B regression trees using B bootstrapped training\n",
    "sets\n",
    "- Average the resulting predictions. These trees are grown deep,\n",
    "and are not pruned. Hence each individual tree has high variance, but\n",
    "low bias. Averaging these B trees reduces the variance. \n",
    "\n",
    "**Bagging on Classification Tree**\n",
    "- For a given test observation, we can record the class predicted by each of the B trees, and\n",
    "take a **majority vote**: the overall prediction is the most commonly occurring\n",
    "class among the B predictions.\n",
    "\n",
    "**B**\n",
    "- In practice we\n",
    "use a value of B sufficiently large that the error has settled down, like B=100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Bag Error Estimation \n",
    "Recall that the key to bagging is that trees are\n",
    "repeatedly fit to bootstrapped subsets of the observations. One can show\n",
    "that on average, each bagged tree makes use of around 2/3 of the\n",
    "observations. The remaining one-third of the observations not used to fit a\n",
    "given bagged tree are referred to as the **out-of-bag (OOB)** observations. \n",
    "\n",
    ">We\n",
    "can predict the response for the ith observation using each of the trees inwhich that observation was OOB. \n",
    "\n",
    "- This will yield around B/3 predictions\n",
    "for the ith observation. \n",
    "- To obtain a single prediction for the ith\n",
    "observation, we can **average** these predicted responses (regression) or can take a **majority vote** (classification). \n",
    "- This leads\n",
    "to a single OOB prediction for the ith observation.\n",
    "\n",
    "The OOB approach for estimating\n",
    "the test error is particularly convenient when performing bagging on large\n",
    "data sets for which **cross-validation** would be computationally onerous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Importance Measures \n",
    "\n",
    "**Bagging improves prediction\n",
    "accuracy at the expense of interpretability**\n",
    "- When we bag a large number of trees, it is no longer possible to represent the resulting statistical learning\n",
    "procedure using a single tree, and it is no longer clear which variables\n",
    "are most important to the procedure\n",
    "\n",
    "\n",
    "\n",
    "**Variable Importance**\n",
    "- One can obtain an overall summary of the importance of\n",
    "each predictor using the RSS (for bagging regression trees) or the Gini index\n",
    "(for bagging classification trees). \n",
    "- **Bagging regression trees**: Record the total amount that the RSS is decreased due to splits\n",
    "over a given predictor, averaged over all B trees. A large value indicates\n",
    "an important predictor. \n",
    "\\begin{align}\n",
    "RSS=\\sum_{j=1}^J\\sum_{i \\in R_j} (y_i-\\hat{y}_{R_j})^2\n",
    "\\end{align}\n",
    "- **Bagging classification\n",
    "trees**: Add up the total amount that the **Gini index** is decreased\n",
    "by splits over a given predictor, averaged over all B trees.\n",
    "\n",
    "<img src=\"./images/11.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "**Random forests** provide an improvement over bagged trees by way of a\n",
    "small tweak that **decorrelates** the trees. \n",
    "\n",
    "As in bagging, we build a number \n",
    "of decision trees on bootstrapped training samples. But when building these\n",
    "decision trees, each time a split in a tree is considered, *a random sample of\n",
    "m predictors is chosen as split candidates* from the full set of p predictors.\n",
    "\n",
    "**The split is allowed to use only one of those m predictors.** A fresh sample of\n",
    "m predictors is taken at each split, and typically we choose $m ≈\\sqrt{p}$ \n",
    "\n",
    "**Rationale**:\n",
    "- Suppose that there is one very strong predictor in the data set, along with a number\n",
    "of other moderately strong predictors. Then in the collection of bagged\n",
    "trees, most or all of the trees will use this strong predictor in the top split.\n",
    "Consequently, *all of the bagged trees will look quite similar to each other.*\n",
    "- Hence the predictions from the bagged trees will be highly correlated. Unfortunately,\n",
    "averaging many highly correlated quantities does not lead to\n",
    "as large of a reduction in variance as averaging many uncorrelated quantities.\n",
    "\n",
    "**Decorrelating** the trees: Random forests forces each split to consider\n",
    "only a subset of the predictors, making the average of the resulting trees less variable\n",
    "and hence more reliable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "**Boosting**: another approach for improving the predictions resulting from a decision tree.\n",
    "- Trees are grown **sequentially**: each tree is grown using information from previously\n",
    "grown trees. \n",
    "- Boosting does not involve bootstrap sampling; instead each\n",
    "tree is fit on a modified version of the original data set.\n",
    "\n",
    "<img src=\"./images/12.png\" width=\"600\">\n",
    "\n",
    "**Idea behind this procedure**\n",
    "- Unlike fitting a single large decision\n",
    "tree to the data, which amounts to fitting the data hard and potentially\n",
    "overfitting, the boosting approach instead **learns slowly**. \n",
    "- Given the current\n",
    "model, we fit a decision tree to the residuals from the model. That is, we\n",
    "fit a tree using the current residuals, rather than the outcome Y , as the response.\n",
    "- We then add this new decision tree into the fitted function in order\n",
    "to update the residuals. Each of these trees can be rather small, with just\n",
    "a few terminal nodes, determined by the parameter **d** in the algorithm. \n",
    "- By\n",
    "fitting small trees to the residuals, we slowly improve $\\hat{f}$ in areas where it\n",
    "does not perform well. \n",
    "- The shrinkage parameter **λ** slows the process down\n",
    "even further, allowing more and different shaped trees to attack the residuals.\n",
    "\n",
    "\n",
    ">Note that in boosting, unlike in bagging, the construction of\n",
    "each tree depends strongly on the trees that have already been grown.\n",
    "\n",
    "**Boosting has three tuning parameters:**\n",
    "1. The number of trees $B$.\n",
    "2. The shrinkage parameter $λ$, a small positive number. This controls the\n",
    "rate at which boosting learns.\n",
    "3. The number $d$ of splits in each tree, which controls the complexity\n",
    "of the boosted ensemble. Often d = 1 works well, in which case each\n",
    "tree is a **stump**, consisting of a single split. In this case, the boosted\n",
    "ensemble is fitting an **additive model**, since each term involves only a\n",
    "single variable. More generally $d$ is the **interaction depth**, and controls\n",
    "the interaction order of the boosted model, since $d$ splits can involve \n",
    "at most d variables.\n",
    "\n",
    "\n",
    "\n",
    "**Boosting V.S. Random forests:**\n",
    "\n",
    "- In boosting, because the growth of a particular tree\n",
    "takes into account the other trees that have already been grown, smaller\n",
    "trees are typically sufficient. \n",
    "- Using smaller trees can aid in interpretability\n",
    "as well; for instance, using **stumps** leads to an additive model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
